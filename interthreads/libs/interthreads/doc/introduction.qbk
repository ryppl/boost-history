[/
  (C) Copyright 2008-20009 Vicente J Botet Escriba.
  Distributed under the Boost Software License, Version 1.0.
  (See accompanying file LICENSE_1_0.txt or copy at
  http://www.boost.org/LICENSE_1_0.txt).
]
[/=============================================================================]

[/============================]
[section:motivation Motivation]
[/============================]

[section Asynchronous Executors and Asynchronous Completion Token Handles]
[/=======================================================================]

In [@http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2005/n1883.pdf N1833 - Preliminary Threading Library Proposal for TR2]
Kevlin Henney introduce the concept of threader an asynchronous executor and a function thread that evaluate a function 
asynchronously and returns an asynchronous completion token joiner, able to join but also to to get the value of the function result.

In [@http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2185.html N2185 - Proposed Text for Parallel Task Execution] 
Peter Dimov introduce a fork function able to evaluate a function asynchronously and 
returns a future handle. 

In [@http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2276.html N2276 - Thread Pools and Futures] 
Anthony William introduce launch_in_thread and launch_in_pool function templates which 
evaluate a function asynchronously either in a specific thread or a thread pool and 
returns a unique_future handle. 

In [@http://www.boostpro.com/vault/index.php?action=downloadfile&filename=boost-threadpool.3.tar.gz&directory=Concurrent%20Programming&  Boost.ThreadPool] 
Oliver Kowalke propose a complete implementation of a thread pool with a submit function 
which evaluate a function asynchronously and returns a task handle. 

Behind all these proposal there is a concept of asynchronous executor, fork-like function and 
the asynchronous completion token handle.

[table fork Parameters
    [
        [[*Proposal]]       [[*executor]]   [[*fork-like]]          [[*ACT handle]]
    ]
    [
        [Boost.Thread]      [??]            [thread constructor]    [thread]
    ]
    [
        [Boost.ThreadPool]  [tp::pool]      [submit]                [tp::task]
    ]
    [
        [N2276]             [thread]        [launch_in_thread]      [unique_future<T>]
    ]
    [
        [N2276]             [thread_pool]   [launch_in_pool]        [unique_future<T>]
    ]
    [
        [N2185]             [??]            [fork]                  [future<T>]
    ]
    [
        [N1833]             [threader]      [thread]                [joiner<T>]
    ]
]

The asynchronous completion token models can follows two interfaces, the thread interface and 
the unique_future interface. Some asynchronous completion token handle allows to recover the result of the evaluation of 
the function, other allows to manage the underlying thread of execution. 

It seems natural to make a generic fork function that will evaluate a function asynchronously 
with respect to the calling thread and returns an ACT handle. The following metafunction  
associated an ACT handle to a asynchronous executor.

    template <typename AE, typename T>
    struct asynchronous_completion_token {
        typedef typename AE::template handle<T>::type type;
    };    

The result of forking a nullary function by an asynchronous executor is given by the metafunction result_of::fork<AE,F>

    namespace result_of { 
        template <typename AE,typename F>
        struct fork {
            typedef typename boost::result_of<F()>::type result_type;
            typedef typename asynchronous_completion_token<AE, result_type>::type type;
        };   
    }

The default implementation of fork delegates on fork asynchronous executor function.

    template< typename AE, typename F > 
    typename result_of::fork<AE, F>::type fork( AE& ae, F fn ) {
        return ae.fork(fn);
    }

Forking n-ary functions relies on the nullary version and bind.

    template< typename AE, typename F, typename A1, ..., typename An > 
    typename asynchronous_completion_token<AE, 
                typename boost::result_of<F(A1,..., An)>::type >::type 
    fork( AE& ae, F fn, A1 a1, ..., An an ) {
        return ae.fork( bind( fn, a1, ..., an ) );
    }

We can define a basic_threader which just returns a new thread as follows:

    class basic_threader {
    public: 
        template <typename T>
        struct handle {
            typedef boost::thread type;
        };

        template <typename F>
        boost::thread fork(F f) {
            thread th(f);
            return boost::move(th);
        }   
    };

The library includes also a launcher class that creates a thread and returns a unique_future when forking 

    class launcher {
    public: 
        template <typename T>
        struct handle {
            typedef unique_future<T> type;
        };
        template <typename F>
        unique_future<typename result_of<F()>::type> 
        fork(F f) {
            typedef typename boost::result_of<F()>::type result_type;
            packaged_task<result_type> tsk(f);
            unique_future<result_type> res = tsk.get_future();
            thread th(boost::move(tsk));
            return res;
        }   
    };

and a shared_launcher class that creates a thread and returns a shared_future when forking 

Given the sequential example:

    double f( double a, int n )
    {
        double r = 0.0;

        for( int i = 1; i <= n; ++i )
        {
            double x = 1.0 / i;
            r += std::pow( x, a );
        }

        return r;
    }

    int main()
    {
        double m1 = f( 1.0, 1000000 );
        double m2 = f( 1.0, 5000000 );
        double m3 = f( 2.2, 1000000 );
        double m4 = f( 2.2, 5000000 );

        std::cout << m2 - m1 + m3 - m4 << std::endl;
    }

this library allows a programmer to switch to parallel execution as follows:

    int main()
    {
        launcher l;
        boost::unique_future<double> fm1 = bith::fork( l, f, 1.0, 1000000 );
        boost::unique_future<double> fm2 = bith::fork( l, f, 1.0, 5000000 );
        boost::unique_future<double> fm3 = bith::fork( l, f, 2.2, 1000000 );
        boost::unique_future<double> fm4 = bith::fork( l, f, 2.2, 5000000 );

        std::cout << fm2.get() - fm1.get() + fm3.get() - fm4.get() << std::endl;
    }


The library include also a threader class based on the Kevlin proposal:

    class unique_threader {
    public: 
        template <typename T>
        struct handle {
            typedef unique_joiner<T> type;
        };
        template <typename F>
        unique_joiner<typename result_of<F()>::type> 
        fork(F f) {
            typedef typename result_of<F()>::type result_type;
            return joiner<result_type>(f);
        }

    };

The question now is how we can adapt it to an existing asynchronous executor such as 
the Boost.ThreadPool library. We need to specialize the template class 
asynchronous_completion_token

    namespace boost { namespace interthreads {
    
    template <typename Channel, typename T>
    struct asynchronous_completion_token<boost::tp::pool<Channel>,T> {
        typedef boost::tp::task<T> type;
    };    
    
    }}

and function fork function.
 
    namespace boost { namespace interthreads {
    
    template< typename Channel, typename F > 
    result_of::fork<boost::tp::pool<Channel>, F>::type  
    fork<boost::tp::pool<Channel>,F>( boost::tp::pool<Channel>& ae, F fn ) {
        return ae.submit(fn);
    }
    }
    }
    
As the preceding is ilegal in C++03 we need to use an auxiliary class to define the default behaviour of fork

    namespace boost { namespace interthreads {
    template< typename AE, typename F > 
    struct fork_aux {
        static typename result_of::fork<AE,F>::type fork(AE& ae, F fn ) {
            return ae.fork(fn);
        }
    };

    template< typename AE, typename F > 
    typename result_of::fork<AE,F>::type 
    fork( AE& ae, F fn ) {
        return fork_aux<AE,F>::fork(ae,fn);
    }
    }
    }

And specialize partially the fork_auc class

    template< typename Channel, typename F > 
    struct fork_aux<boost::tp::pool<Channel>,F> {
        typename result_of::fork<boost::tp::pool<Channel>, F>::type  
        fork( boost::tp::pool<Channel>& ae, F fn ) {
            return ae.submit(fn);
        }
    };



Note that the single fork function that needs specialization is the one taking a nullary 
function as parameter.

We can write the preceding main function in a more generic way

    template < typename AE>
    void do(AE& ae)
    {
        typedef bith::result_of::fork<AE, int(*)(double, int) >::type auto_type;
        auto_type fm1 = bith::fork(ae, f, 1.0, 1000000 );
        auto_type fm2 = bith::fork(ae, f, 1.0, 5000000 );
        auto_type fm3 = bith::fork(ae, f, 2.2, 1000000 );
        auto_type fm4 = bith::fork(ae, f, 2.2, 5000000 );

        std::cout << fm2.get() - fm1.get() + fm3.get() - fm4.get() << std::endl;
    }

    int main()
    {
        launcher ae;
        do(ae);
    }

and we can switch from using the launcher or the tp::pool just by changing one line

    int main()
    {
        boost::tp::pool<> ae(boost::tp::poolsize(6))
        do(ae);
    }

The library allows also to fork several functions at the same time

    result_of::fork_all<AE, int(*)(), int(*)(), int(*)()>::type handles = bith::fork_all(ae, f, g, h);
    std::cout << get<1>(res).get() - get<0>(res).get() + get<2>(res).get() << std::endl;


The result of the fork_all operation is a fusion tuple of asynchronous completion token handles. 
The user can apply any fusion algorithm on this tuple as for example

    bool b = fusion::none(handles, fct::interruption_requested());

The asynchronous completion token models follows two interfaces, the thread interface and the 
unique_/shared_future interface.

To make common tasks easier the library provide some functors in the name space fct: 
for the thread interface as 

* fct::join 
* fct::join_until
* fct::join_for 
* fct::detach 
* fct::interrupt 
* fct::interrupt_requested

and for the future operations as

* fct::get
* fct::wait
* fct::wait_until
* fct::wait_for
* fct::is_ready
* fct::has_value
* fct::has_exception

Here is an example for get:

    namespace fct {
        struct get {
            template<typename ACT>
            typename ACT::result_type operator()(ACT& t) const {
                return t.get();
            }
        };
    }

In addition the library provides some non member functions that are the result of applying 
these functors to the tuple using a fusion algorithm:

* join_all
* join_all_until
* join_all_for 
* detach_all
* interrupt_all
* interrupt_requested_on_all

* get_all
* wait_all
* wait_all_until
* wait_all_for
* are_all_ready
* have_all_value
* have_all_exception

Next follows how get_all is defined.

    template <typename MovableTuple>
    typename result_of::get_all<Sequence>::type
    get_all(Sequence& t) {
        return fusion::transform(t, fct::get());
    }

The library defines in a systematic way the result_of of a function as a metafunction 
having the same name as the function on the namespace result_of, as the Boost.Fusion library 
does.

    namespace result_of {
        template <typename Sequence>
        struct get_all {
            typedef typename fusion::result_of::transform<Sequence, fct::get>::type type
        };
    }

So the user can do the following

    result_of::fork_all<AE, int(*)(), int(*)(), int(*)()>::type res = bith::fork_all(ae, f, g, h);
    result_of::get_all<result_of::fork_all<AE, int(*)(), int(*)(), int(*)()>::type>::type values 
        = bith::get_all(handles);

or using a typedef

    typedef result_of::fork_all<AE, int(*)(), int(*)(), int(*)()>::type auto_type;
    auto_type handles = bith::fork_all(ae, f, g, h);
    result_of::get_all<auto_type>::type values= bith::get_all(handles);

Note that the notation can be shortened by using the C++0x auto keyword.

    auto res = bith::fork_all(ae, f, g, h);
    auto values = bith::get_all(handles);

Last but not least the library provides also some sugaring functions like 
wait_for_all that forks and wait for the result.

    result_of::wait_for_all<AE, int(*)(), int(*)(), int(*)()>::type res = bith::wait_for_all(ae, f, g, h);
    std::cout << get<1>(res) - get<0>(res) + get<2>(res) << std::endl;

and wait_for_any which works with functions that return the same type or are convertible to the same type.

    result_of::wait_for_any<AE, int(*)(), int(*)(), int(*)()>::type res = bith::wait_for_any(ae, f, g, h);
    std::cout << "function " << res.first << " finshed first with result=" << res.second << std::endl;

The current implementation use the wait_for_any function so any AE must provide a way to get a 
unique|shared_future from its ACT.

The library defines a functor allowing the user to specialize it

    template <typename AE>
    struct get_future {
        template <typename T>
        shared_future<T>& operator()(typename asynchronous_completion_token<AE,T>::type& act) 
        { return act.get_future(); }
    };

Future versions will use the set_once synchronization.

Resuming a simple way to define a new AsynchronousExecutor is to define a class as

    struct AsynchronousExecutor {
        template <typename T>
        struct handle {
            typedef implementation-specific type;
        };
        
        template <typename F>
        typename handle<typename result_of<F()>::type>::type 
        fork(F f);
    };




[endsect]

[section Threader/Joiner]
[/=============================================================================]

See the [@http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2005/n1883.pdf N1833 - Preliminary Threading Library Proposal for TR2]
where Kevlin Henney introduce the concept of threader an asynchronous executor and a function thread that evaluate a function 
asynchronously and returns an asynchronous completion token joiner, able to join but also to to get the value of the function result.

The main specifcities is that here we make a difference between unique_joiner (move-only) and shared_joiner and as consequence unique_threader and shared_threader.


[endsect]

[section Decorators]
[/=============================================================================]

`boost::call_once` provides a mechanism for ensuring that an initialization routine is run exactly once on a 
programm without data races or deadlocks. 
`boost::this_thread::at_thread_exit` allows to execute a cleanup function at thread exit.

If we want a setup function be executed once at the begining on the threads and a cleanup at thread exit we need to do

    void thread_main() {
        setup();
        boost::this_thread::at_thread_exit(cleanup);
        // do whatever
        // ...
    }
    // ...
    {
        launch(thread_main);
        //...
    }

Of course we can define an init function that call setup and do the registration.

    void init() {
        setup();
        boost::this_thread::at_thread_exit(cleanup);
    }

Different services could require these setup/cleanup functions to be called, and so 
each thread function should do 

    void thread_main() {
        serv1::init();
        // ...
        servN::init();
        // do whatever using serv1, ..., servN.
        // ...
    }

This approach is valid for services that the user can configure for specifics threads, 
but not for services that must be installed on every thread.

__thread_decoration__ ensures that a setup function is called only once by thread before 
the thread function provided the thread is created with a decorator wrapper. 
This setup function is usualy used to set thread specific pointers and call functions once.

The conterpart of the setup is the cleanup. The __thread_decoration__ takes an optional 
cleanup function which will be executed at thread exit.

    // define in only the implementation file of each service
    
    boost::interthreads::decoration serv1:decoration(serv1:setup, serv1:cleanup);
    // ...
    boost::interthreads::decoration servN:decoration(servN:setup, servN:cleanup);
    
    
    void thread_main() {
        // do whatever using serv1, ..., servN.
        // ...
    }
    
    // ...
    {
        boost::thread th(boost::interthreads::make_decorator(thread_main));
        //...
    }
    
We can use a basic_threader_decorator as asynchronous executor to fork thread_main.    
    // ...
    {
        boost::thread th=fork(basic_threader_decorator(), thread_main);
        //...
    }


[endsect]

[section Sharing Thread Local Storage]
[/=============================================================================]

Thread local storage allows multi-threaded applications to have a separate instance of a given data item for 
each thread. But do not provide any mechanism to access this data from other threads. Although this seems to 
defeat the whole point of thread-specific storage, it is useful when these contexts needs some kind of 
communication between them, or some central global object needs to control them.

The intent of the `boost::thread_specific_shared_ptr` class is to allow two threads to establish a shared memory 
space, without requiring the user code to pass any information.
`boost::thread_specific_shared_ptr` provides a portable mechanism for shared thread-local storage that works on 
all compilers supported by `boost::thread` and `boost::shared_ptr`. Each instance of 
`boost::thread_specific_shared_ptr` represents a pointer to a shared object where each thread must have a distinct 
value. 

Only the current thread can modify the thread specific shared pointer using the non const functions reset/release 
functions. Each time these functions are used a synchronization must be ensured to update the mapping.
The other threads have only read access to the shared_ptr<T>. It is worh saying that the shared object T must be 
thread safe.

[endsect]

[section:keep_alive_motivation Keep Alive]

On fault tolerant systems we need to be able to detect threads that could stay on a loop, or simply blocked.

One way to detect this situations is to require the thread to signal it is alive by calling a check point function.
Of course it should be up to the user to state when this mechanism is enabled or disabled. 
At the begining of a thread the keep alive mechanism is disabled.

A thread will be considered dead if during a given period the number of checkins is inferior to a given threshold.
These two parameters can be given when the keep alive mechanislm is enabled.

The controler checks at predefined intervals if the thread is dead, and in this case it will call a user specific 
function which by default aborts the program.

[endsect]

[section Thread Tuple]
[/=============================================================================]

The __thread_group__ class allows to group dynamically threads. This means that the container must be dynamic.

    {
        boost::thread_group tg;
        tg.create_thread(thread1);
        tg.create_thread(thread2);
        tg.join_all(thread1);
    }


The __thread_tuple__ class is responsible for launching and managing a static collection of threads 
that are related in some fashion. No new threads can be added to the tuple once constructed. So we can write

    {
        bith::thread_tuple<2> tt(thread1, thread2);
        tt.join_all(thread1);
    }

As this
    bith::conc_join_all(thread1, thread2);

In addition the user can join the first finishing thread.

    unsigned i = bith::conc_join_any(thread1, thread2);


Evidently, thread_tuple can not be used when we needs dynamic creation or deletion. The __thread_group__ class allows to group dynamically threads.

    {
        boost::thread_group tg;
        tg.create_thread(thread1);
        
        // later on
        tg.create_thread(thread2);
        boost::thread th3(thread3)
        tg.add_thread(th3);
        
        // later on
        tg.remove_thread(th3);
        
        tg.join_all(thread1);
    }

Objects of type __thread_tuple__ are movable, so they can be stored in move-aware containers, and returned from 
functions. This allows the details of thread tuple creation to be wrapped in a function.

    boost::interthreads::thread_tuple<2> make_thread_tuple(...);

    void f()
    {
        bith::thread_tuple<2> some_thread_tuple=bith::make_thread_tuple(f1, g2);
        some_thread_tuple.join();
    }

[endsect]



[endsect]
