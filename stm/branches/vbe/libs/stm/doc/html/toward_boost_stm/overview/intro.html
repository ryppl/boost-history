<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<title>Introduction</title>
<link rel="stylesheet" href="../../../../../../doc/html/boostbook.css" type="text/css">
<meta name="generator" content="DocBook XSL Stylesheets V1.73.2">
<link rel="start" href="../../index.html" title="Chapter 1. Toward.Boost.STM">
<link rel="up" href="../overview.html" title="Overview">
<link rel="prev" href="../overview.html" title="Overview">
<link rel="next" href="../users_guide.html" title="Users'Guide">
</head>
<body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF">
<table cellpadding="2" width="100%"><tr>
<td valign="top"><img alt="TowardBoostSTM" width="277" height="86" src="../../../image/Toward_Boost_STM.jpg"></td>
<td align="center"><a href="../../../../../../index.html">Home</a></td>
<td align="center"><a href="../../../../../../libs/libraries.htm">Libraries</a></td>
<td align="center"><a href="http://www.boost.org/users/people.html">People</a></td>
<td align="center"><a href="http://www.boost.org/users/faq.html">FAQ</a></td>
<td align="center"><a href="../../../../../../more/index.htm">More</a></td>
</tr></table>
<hr>
<div class="spirit-nav">
<a accesskey="p" href="../overview.html"><img src="../../../../../../doc/html/images/prev.png" alt="Prev"></a><a accesskey="u" href="../overview.html"><img src="../../../../../../doc/html/images/up.png" alt="Up"></a><a accesskey="h" href="../../index.html"><img src="../../../../../../doc/html/images/home.png" alt="Home"></a><a accesskey="n" href="../users_guide.html"><img src="../../../../../../doc/html/images/next.png" alt="Next"></a>
</div>
<div class="section" lang="en">
<div class="titlepage"><div><div><h3 class="title">
<a name="toward_boost_stm.overview.intro"></a><a class="link" href="intro.html" title="Introduction"> Introduction</a>
</h3></div></div></div>
<a name="toward_boost_stm.overview.intro.transactional_memory"></a><h5>
<a name="id4848945"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.transactional_memory">Transactional
        Memory</a>
      </h5>
<p>
        Transactional memory is a modern type of concurrency control that uses transactions
        as its synchronization mechanism. A transaction is a finite sequence of operations
        that are executed in an atomic, isolated and consistent manner. The atomicity,
        isolation and consistency (ACI) of transaction's are derived from the ACID
        principle in the database community. TM does not exhibit the D (durability)
        of ACID because unlike database transactions, TM transactions are not saved
        to permanent storage (e.g., hard drives).
      </p>
<p>
        Transactions are executed speculatively (optimistically) and are checked
        for consistency at various points in the transaction's lifetime. Programmers
        specify the starting and ending points of a transaction. All of the operations
        between those points make up the transaction's execution body. Transactions
        are commonly represented using the atomic structure shown in the figure below.
      </p>
<pre class="programlisting"><span class="number">1</span>   <span class="identifier">atomic</span>
<span class="number">2</span>   <span class="special">{</span>
<span class="number">3</span>       <span class="special">++</span><span class="identifier">x</span><span class="special">;</span>
<span class="number">4</span>       <span class="special">--</span><span class="identifier">y</span><span class="special">;</span>
<span class="number">5</span>   <span class="special">}</span>

<span class="identifier">Simple</span> <span class="identifier">Transaction</span> <span class="identifier">Using</span> <span class="identifier">the</span> <span class="identifier">atomic</span> <span class="identifier">Keyword</span>
</pre>
<p>
        Once a transaction has started it either commits or aborts. A transaction's
        operations are only seen once the transaction has committed, providing the
        illusion that all of the operations occurred at a single instance in time.
        The instructions of a committed transaction are viewed as if they occurred
        as an indivisible event, not as a set of operations executed serially. The
        operations of an aborted transaction are never seen by other threads, even
        if such operations were executed within a transaction and then rolled back.
      </p>
<p>
        In the case of the above example, if the transaction was committed both operations
        <code class="computeroutput"><span class="special">++</span><span class="identifier">x</span></code>
        and <code class="computeroutput"><span class="special">--</span><span class="identifier">y</span></code>
        would be made visible to other threads at the same instance in time. If the
        transaction the above example was aborted, neither operation (<code class="computeroutput"><span class="special">++</span><span class="identifier">x</span></code> and
        <code class="computeroutput"><span class="special">--</span><span class="identifier">y</span></code>)
        would appear to have occurred even if the local transaction executed one
        or both operations.
      </p>
<p>
        TM offers three distinct advantages over other parallel programming abstractions.
      </p>
<div class="orderedlist"><ol type="1">
<li>
          TM is simple; transactions, the synchronization mechanism of TM, are easier
          to program than other synchronization mechanisms because they move shared
          memory management into the underlying TM subsystem, removing its complexity
          from the programmer's view. Moreover, TM exposes a simple programmer interface,
          reducing (or in some cases, removing) the potential for deadlock, livelock
          and priority inversion.
        </li>
<li>
          TM is scalable; it achieves increased computational throughput when compared
          to other parallel programming abstractions by allowing multiple threads
          to speculatively execute the same critical section. When concurrently executing
          threads do not exhibit shared data conflicts, they are guaranteed to make
          forward progress.
        </li>
<li>
          TM is modular; transactions can be nested to any depth and function as
          a single unit. This behavior allows application programmers to extend atomic
          library algorithms into atomic domain-specific algorithms without requiring
          the application programmers to understand the implementation details of
          the library algorithm.
        </li>
</ol></div>
<p>
        For these reasons, transactions are considered an important synchronization
        mechanism and TM is viewed as an important type of concurrency control. The
        remainder of this section presents TM from a viewpoint of (1) simplicity,
        (2) scalability and (3) modularity.
      </p>
<a name="toward_boost_stm.overview.intro.simplicity"></a><h5>
<a name="id4849486"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.simplicity">Simplicity</a>
      </h5>
<p>
        Synchronization problems, such as deadlocks, livelocks and priority inversion
        are common in software systems using mutual exclusion. TM avoids many of
        these problems by providing a synchronization mechanism that does not expose
        any of its implementation details to the programmer. The only interfaces
        the programmer needs to use for TM is as follows:
      </p>
<div class="itemizedlist"><ul type="disc">
<li>
          begin_tx() - the signaled start of the transaction.
        </li>
<li>
          read(loc) - reads the specified memory location, storing its location in
          the transaction's read set and returning its current value.
        </li>
<li>
          write(loc, val) - writes the specified memory location to the supplied
          val, storing its location in the transaction's write set.
        </li>
<li>
          end_tx() - the signaled end of the transaction. end_tx() returns true if
          the transaction commits, otherwise it returns false.
        </li>
</ul></div>
<p>
        The above interfaces allow the programmer to create a transaction (using
        begin_tx()), specify its memory operations (using read() and write()) and
        terminate (using end_tx()()). Moreover, none of the interfaces specify details
        of the TM subsystem's implementation. This leaves the TM system's implementation
        disjoint from the interfaces it supplies, a key characteristic for TM's simplicity.
      </p>
<p>
        All TM implementations use some combination of the above interfaces. TMs
        implemented within compilers tend to implicitly annotate transactional read()
        and write() operations, whereas those implemented within software libraries
        tend to require the programmer explicitly state which operations are transactional
        reads and writes. An example of a transaction using the above interfaces
        alongside an actual STM library implementation is shown in Figure 3.
      </p>
<pre class="programlisting"><span class="number">1</span>   <span class="comment">// TM interfaces            // __Boost_STM__
</span><span class="number">2</span>   <span class="keyword">do</span> <span class="special">{</span>                        <span class="identifier">atomic</span><span class="special">(</span><span class="identifier">t</span><span class="special">)</span>
<span class="number">3</span>       <span class="identifier">begin_tx</span><span class="special">();</span>             <span class="special">{</span>
<span class="number">4</span>       <span class="identifier">write</span><span class="special">(</span><span class="identifier">x</span><span class="special">,</span> <span class="identifier">read</span><span class="special">(</span><span class="identifier">x</span><span class="special">)+</span><span class="number">1</span><span class="special">);</span>        <span class="special">++</span><span class="identifier">t</span><span class="special">.</span><span class="identifier">write</span><span class="special">(</span><span class="identifier">x</span><span class="special">);</span>
<span class="number">5</span>       <span class="identifier">write</span><span class="special">(</span><span class="identifier">y</span><span class="special">,</span> <span class="identifier">read</span><span class="special">(</span><span class="identifier">y</span><span class="special">)-</span><span class="number">1</span><span class="special">);</span>        <span class="special">--</span><span class="identifier">t</span><span class="special">.</span><span class="identifier">write</span><span class="special">(</span><span class="identifier">y</span><span class="special">);</span>
<span class="number">6</span>   <span class="special">}</span> <span class="keyword">while</span> <span class="special">(</span><span class="identifier">end_tx</span><span class="special">());</span>         <span class="special">}</span> <span class="identifier">before_retry</span> <span class="special">{}</span>

<span class="identifier">Figure</span> <span class="number">3.</span> <span class="identifier">Transaction</span> <span class="identifier">Using</span> <span class="special">(</span><span class="number">1</span><span class="special">)</span> <span class="identifier">Explicit</span> <span class="identifier">TM</span> <span class="identifier">Interfaces</span> <span class="keyword">and</span> <span class="special">(</span><span class="number">2</span><span class="special">)</span> TBoost.STM<span class="special">.</span>
</pre>
<p>
        Figure 3 implements the same transaction as shown in Figure 2, except all
        transactional memory accesses, including the transaction's retry behavior
        (e.g., its loop), are demonstrated from a simple TM interface perspective
        and an actual library implementation (TBoost.STM). While most TM systems
        handle some portion of these interface calls implicitly, as is shown in the
        TBoost.STM transaction, it is important to note that even when all operations
        are made visible to the end programmer, transactions are still devoid of
        many concurrency problems, such as data races and deadlocks (explained below),
        that plague other types of concurrency control.
      </p>
<p>
        For example, as long as the programmer properly annotates the access to the
        shared variables x and y as shown in Figure 3, it is impossible for race
        conditions or deadlocks to occur. Furthermore, the programmer does not need
        any program-specific knowledge to use shared data; he or she simply uses
        the TM interfaces supplied by the system and the resulting behavior is guaranteed
        to be consistent. This is explained in greater detail in Section 3.1.
      </p>
<p>
        Other types of concurrency control, such as mutual exclusion, cannot achieve
        the same interface simplicity, because part of their implementation is associated
        with, or exposed through, their interface. To demonstrate this, consider
        the fine-grained locking example of Figure 1 as shown below.
      </p>
<pre class="programlisting"><span class="number">1</span>   <span class="comment">// fine-grained locking
</span><span class="number">2</span>   <span class="identifier">lock</span><span class="special">(</span><span class="identifier">mutexX</span><span class="special">);</span>
<span class="number">3</span>   <span class="identifier">lock</span><span class="special">(</span><span class="identifier">mutexY</span><span class="special">);</span>
<span class="number">4</span>   <span class="special">++</span><span class="identifier">x</span><span class="special">;</span>
<span class="number">5</span>   <span class="special">--</span><span class="identifier">y</span><span class="special">;</span>
<span class="number">6</span>   <span class="identifier">unlock</span><span class="special">(</span><span class="identifier">mutexX</span><span class="special">);</span>
<span class="number">7</span>   <span class="identifier">unlock</span><span class="special">(</span><span class="identifier">mutexY</span><span class="special">);</span>
</pre>
<p>
        There is no universal interface that can be used to properly access the shared
        data protected by the mutual exclusion in the above fine-grained locking
        example. Instead, the programmer must be aware that mutexX and mutexY protect
        shared data x and y and, therefore, the locks must be obtained before accessing
        the shared data. In short, the programmer is responsible for knowing not
        only that mutual exclusion is used, but also how it is used (e.g., which
        locks protect which shared variables). In this case, mutexX must be obtained
        before mutexY. If another section of code implements the following, a deadlock
        scenario will eventually occur.
      </p>
<pre class="programlisting"><span class="number">1</span>   <span class="comment">// fine-grained locking
</span><span class="number">2</span>   <span class="identifier">lock</span><span class="special">(</span><span class="identifier">mutexY</span><span class="special">);</span>
<span class="number">3</span>   <span class="identifier">lock</span><span class="special">(</span><span class="identifier">mutexX</span><span class="special">);</span> <span class="comment">// deadlock here
</span><span class="number">4</span>   <span class="special">--</span><span class="identifier">y</span><span class="special">;</span>
<span class="number">5</span>   <span class="special">++</span><span class="identifier">x</span><span class="special">;</span>
<span class="number">6</span>   <span class="identifier">unlock</span><span class="special">(</span><span class="identifier">mutexY</span><span class="special">);</span>
<span class="number">7</span>   <span class="identifier">unlock</span><span class="special">(</span><span class="identifier">mutexX</span><span class="special">);</span>
</pre>
<a name="toward_boost_stm.overview.intro.understanding_concurrency_hazards"></a><h5>
<a name="id4804185"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.understanding_concurrency_hazards">Understanding
        Concurrency Hazards</a>
      </h5>
<p>
        Informally, a concurrency hazard is a condition existing in a set of operations
        that, if executed with a specific concurrent interleaving, results in one
        or many unwanted sideeffects. Most errors in parallel systems, such as deadlocks
        and priority inversion, are the specific execution of concurrency hazards
        resulting in the unwanted side-effect(s) they contain. If the concurrency
        hazards are eliminated, the parallel system errors contained within the concurrency
        hazards are also eliminated. Unfortunately, detecting existing concurrency
        hazards is non-trivial and therefore eliminating them is also non-trivial.
      </p>
<p>
        Mutual exclusion exhibits more concurrency hazards than TM because its implementation
        details (i.e., its locks) must be exposed and used by the end programmer.
        While the locks used to enforce mutual exclusion by themselves are not concurrency
        hazards, their use can lead to a number of hazards. As such, using locks
        leads to concurrency hazards.
      </p>
<p>
        Because the mutual exclusion locking details are exposed to the programmer
        and because the programmer must maintain a universal and informal contract
        to use these locks, concurrency hazards can arise due to the number of possible
        misuses that can be introduced by the programmer. In particular, if the programmer
        accidentally deviates from the informal locking contract, he or she may inadvertently
        introduce a concurrency hazard that can cause the program to deadlock, invert
        priority or lead to inconsistent data.
      </p>
<p>
        In contrast, TM has no universal or informal contract between shared data
        that the end programmer needs to understand and follow as is required in
        mutual exclusion. Due to this, TM can hide its implementation details which
        results in reduced concurrency hazards. In particular, each transaction tracks
        the memory it uses in its read and write sets. When a transaction begins
        its commit phase, it verifies its state is consistent and commits its changes.
        If a transaction finds its state is inconsistent, it discards its changes
        and restarts. All of this can be achieved using the basic TM interfaces shown
        in Section 3 without exposing any implementation details. In order to use
        TM, the end programmer only needs to know how to correctly create a transaction.
        Once the transaction is executed, regardless of how it is executed, it results
        in a program state that is guaranteed to be consistent.
      </p>
<p>
        Fundamentally, TM exhibits less concurrency hazards than mutual exclusion
        because its implementation details are divorced from its interface and can
        therefore be hidden within its subsystem. Any number of implementations can
        be used in a TM subsystem using only the basic TM interfaces shown in Section
        3. The same is not true for mutual exclusion. Mutual exclusion, regardless
        of how it is implemented, exposes details of its implementation to the programmer.
        As demonstrated in Section 5, mutual exclusion does not provide software
        modularity specifically because extending an existing module requires an
        understanding and extension of that module's implementation. When such locking
        implementations are hidden inside of software libraries, extending these
        modules can range from difficult to impossible.
      </p>
<a name="toward_boost_stm.overview.intro.testing__race_conditions_and_interleavings"></a><h5>
<a name="id4804299"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.testing__race_conditions_and_interleavings">Testing:
        Race Conditions and Interleavings</a>
      </h5>
<p>
        A race condition is a common concurrency hazard that exists in parallel or
        distributed software. As with all concurrency hazards, race conditions rely
        on a specific interleaving of concurrent execution to cause their unwanted
        side-effect. In this section we demonstrate that race conditions do not exist
        in TM and therefore, software testing is greatly simplified because all possible
        interleavings do not need to be tested to ensure correct system behavior.
        In order to demonstrate that race conditions are absent from TM, we must
        first show that they are present in other types of concurrency control.
      </p>
<pre class="programlisting"><span class="number">1</span>   <span class="comment">// Thread T1 // Thread T
</span><span class="number">2</span>   <span class="identifier">lock</span><span class="special">(</span><span class="identifier">L2</span><span class="special">);</span>
<span class="number">3</span>   <span class="identifier">lock</span><span class="special">(</span><span class="identifier">L1</span><span class="special">);</span>
<span class="number">4</span>   <span class="special">...</span>
<span class="number">5</span>   <span class="identifier">unlock</span><span class="special">(</span><span class="identifier">L1</span><span class="special">);</span>
<span class="number">6</span>   <span class="identifier">unlock</span><span class="special">(</span><span class="identifier">L2</span><span class="special">);</span>
<span class="number">7</span>   <span class="identifier">lock</span><span class="special">(</span><span class="identifier">L1</span><span class="special">);</span>
<span class="number">8</span>   <span class="identifier">lock</span><span class="special">(</span><span class="identifier">L2</span><span class="special">);</span>
<span class="number">9</span>   <span class="special">...</span>

<span class="identifier">Figure</span> <span class="number">4.</span> <span class="identifier">Mutual</span> <span class="identifier">Exclusion</span> <span class="identifier">Race</span> <span class="identifier">Condition</span><span class="special">.</span>
</pre>
<p>
        Consider the race condition present in the mutual exclusion example shown
        in Figure 4. The race condition present in the example results in a deadlock
        if thread T1 executes line 7 followed by thread T2 executing line 2. However,
        if the program executes the lines in order (e.g., line 1, then line 2, then
        line 3, etc.), the system will execute properly. The fundamental problem
        in Figure 4 is that it contains a concurrency hazard; in particular, it contains
        a race condition. To further complicate matters, the race condition can only
        be observed in two of many possible concurrent executions. Those two executions
        are: T1 executes line 7 followed by T2 executing line 2 or T2 executes line
        2 followed by T1 executing line 7. All other possible concurrent interleavings
        of threads T1 and T2 avoid the deadlock race condition. More specifically,
        as long as T1 executes lines 7-8 atomically or T2 executes line 2-3 atomically,
        all remaining concurrent interleavings are free of the deadlock race condition.
      </p>
<p>
        Because it is unlikely that the deadlock race condition will occur, the programmer
        may never observe it, no matter how many times the program is tested. Only
        exhaustive testing, which tests all possible concurrent interleavings, is
        guaranteed to identify the presence of the deadlock. Regrettably, exhaustive
        testing is an unrealistic solution for most programs due to the time it would
        take to execute all possible concurrent interleavings of the program.
      </p>
<p>
        An alternative to exhaustive testing is for programmers to use types of concurrency
        control that are devoid of certain concurrency hazards. For example, if mutual
        exclusion did not emit the race condition concurrency hazard, it would be
        impossible for a program using it to deadlock. Therefore, exhaustive testing
        would not be necessary. While this scenario is hypothetical, it illustrates
        our larger argument: in order to avoid common parallel problems in a practical
        fashion, programmers may need to only use types of concurrency control that
        are devoid of certain concurrency hazards. By doing this, the program using
        the specific type of concurrency control will be guaranteed to be free of
        certain common parallel problems.
      </p>
<p>
        TMs are required to be devoid of race conditions within their implementations
        because they must enforce the ACI (atomic, consistent and isolated) principles.
        Transactions must execute as atomic and isolated and, therefore, TMs are
        not capable of supporting concurrent interleavings between multiple transactions
        as that would violate the atomic and isolated principles of ACI. Due to this,
        programs only using TM are guaranteed to be free of deadlocks (i.e., deadlockfreedom).
        Moreover, because TM implementations can guarantee freedom of race condition
        concurrency hazards, programmers only need to verify their transactional
        code is correct in a sequential (non-parallel) manner. Once the sequential
        execution of the transactional code has been verified, no more testing is
        required as the TM system is required to behave in a consistent manner for
        all serial orders.
      </p>
<a name="toward_boost_stm.overview.intro.development__mutual_exclusion_and_tm"></a><h5>
<a name="id4804665"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.development__mutual_exclusion_and_tm">Development:
        Mutual Exclusion and TM</a>
      </h5>
<p>
        The development of fine-grained locking is notoriously difficult. Designing
        such software is equally as hard. The difficulty in developing and designing
        fine-grained locking systems is rooted in conflicting heuristics. A primary
        goal of software design is to identify the most simplistic software solution
        that exists for a particular problem. A primary goal of fine-grained locking
        is the most efficient concurrent implementation of a software system. The
        goals of software design and fine-grained locking are conflicting because
        the most efficient fine-grained locking solution usually requires some of
        the most complex software design implementations to achieve such performance.
      </p>
<p>
        TM achieves scalability by using optimistic concurrency that is implemented
        within its subsystem (see Section 4). Since the TM subsystem is the efficiency
        throttle for TM, which is unexposed to the programmer, the software architecture
        and design never needs to be complicated (nor can it be) in order to achieve
        increased parallelism when using transactions. As will be demonstrated in
        the following section, transactions run efficiently using the interfaces
        shown in this section and are never complicated in order to achieve improved
        performance, as is commonly found in fine-grained mutual exclusion implementations.
      </p>
<a name="toward_boost_stm.overview.intro.scalability"></a><h5>
<a name="id4804718"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.scalability">Scalability</a>
      </h5>
<p>
        In this section we analyze the scalability of TM compared to mutual exclusion.
        We measure scalability by two metrics: consistency and performance. A concurrency
        control type has consistent scalability if it guarantees correct behavior
        for an arbitrarily large number of concurrently executing processes.4 Performance
        scalability is measured by the maximum number of consistent processes supported
        by a concurrency control type while executing concurrently.
      </p>
<a name="toward_boost_stm.overview.intro.pessimistic_and_optimistic_critical_sections"></a><h5>
<a name="id4804741"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.pessimistic_and_optimistic_critical_sections">Pessimistic
        and Optimistic Critical Sections</a>
      </h5>
<p>
        Critical sections can be pessimistic or optimistic. Pessimistic critical
        sections limit their critical section execution to a single thread. Locks
        are an example of a synchronization mechanism that use pessimistic critical
        sections. Optimistic critical sections allow unlimited concurrent threaded
        execution. Transactions are an example of a synchronization mechanism that
        use optimistic critical sections.
      </p>
<a name="toward_boost_stm.overview.intro.truly_optimistic_critical_sections"></a><h5>
<a name="id4804773"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.truly_optimistic_critical_sections">Truly
        Optimistic Critical Sections</a>
      </h5>
<p>
        Truly optimistic critical sections are those critical sections which allow
        multiple conflicting threads to simultaneously execute the same critical
        section. A deferred update (or lazy acquire) TM system supports truly optimistic
        critical section. A direct update (or eager acquire) TM system does not support
        truly optimistic critical sections. More details on deferred and direct update
        TM systems are presented in the subsequent sections.
      </p>
<p>
        Truly optimistic critical sections are important because they allow simultaneous
        conflicting critical section execution, as opposed to disallowing such behavior.
        It is important to allow conflicting critical section execution because prematurely
        preventing concurrently executing threads pessimistically degrades performance.
        To demonstrate this, consider two transactions, called T1 and T2, executing
        the same critical section. Transaction T1 starts first and tentatively writes
        to memory locationM. Transaction T2 then starts and tries to write to memory
        locationM. In a truly optimistic TM system, T2 would be allowed to tentatively
        write to location M while T1 is also writing to M. This behavior then allows
        T2 to commit before T1 in the event T2 completes before T1.
      </p>
<p>
        In comparison, if the TM system is not truly optimistic, once T1 writes to
        M, T2 must stall until T1 completes. This pessimistically degrades the performance
        of the system by prematurely deciding that T1's transactional execution should
        have higher priority than T2's.
      </p>
<pre class="programlisting"><span class="number">1</span>   <span class="comment">// global variables
</span><span class="number">2</span>   <span class="keyword">int</span> <span class="identifier">g1</span> <span class="special">=</span> <span class="number">0</span><span class="special">;</span> <span class="keyword">int</span> <span class="identifier">g2</span> <span class="special">=</span> <span class="number">0</span><span class="special">;</span>
<span class="number">3</span>
<span class="number">4</span>   <span class="keyword">void</span> <span class="identifier">set1</span><span class="special">(</span><span class="keyword">int</span> <span class="identifier">val</span><span class="special">)</span> <span class="special">{</span> <span class="identifier">atomic</span> <span class="special">{</span> <span class="identifier">g1</span> <span class="special">=</span> <span class="identifier">val</span><span class="special">;</span> <span class="special">}</span> <span class="special">}</span>
<span class="number">5</span>   <span class="keyword">void</span> <span class="identifier">set2</span><span class="special">(</span><span class="keyword">int</span> <span class="identifier">val</span><span class="special">)</span> <span class="special">{</span> <span class="identifier">atomic</span> <span class="special">{</span> <span class="identifier">g2</span> <span class="special">=</span> <span class="identifier">val</span><span class="special">;</span> <span class="special">}</span> <span class="special">}</span>
<span class="number">6</span>   <span class="keyword">int</span> <span class="identifier">get1</span><span class="special">()</span> <span class="special">{</span> <span class="identifier">atomic</span> <span class="special">{</span> <span class="keyword">return</span> <span class="identifier">g1</span><span class="special">;</span> <span class="special">}</span> <span class="special">}</span>
<span class="number">7</span>   <span class="keyword">int</span> <span class="identifier">get2</span><span class="special">()</span> <span class="special">{</span> <span class="identifier">atomic</span> <span class="special">{</span> <span class="keyword">return</span> <span class="identifier">g2</span><span class="special">;</span> <span class="special">}</span> <span class="special">}</span>

<span class="identifier">Figure</span> <span class="number">5.</span> <span class="identifier">Truly</span> <span class="identifier">Optimistic</span> <span class="identifier">Concurrency</span> <span class="identifier">Diagram</span><span class="special">.</span>
</pre>
<p>
        Furthermore, and perhaps more importantly, truly optimistic critical sections
        allow readers and writers of the same memory location to execute concurrently.
        This behavior is important because in many cases, both the readers and writers
        of the same memory can commit with consistent views of memory.
      </p>
<p>
        An example of this is shown in Figure 5. As demonstrated in Figure 5 thread
        1 and thread 2, which we'll refer to as T1 and T2 respectively, operate on
        the same memory locations (g1 and g2). Because the TM system supports optimistic
        concurrency, T2 is allowed to execute concurrently alongside T1 even though
        their memory accesses conflict. However, in this scenario, because T2 completes
        its workload before T1, both transactions are allowed to commit. T2 captures
        the state of g1=0,g2=0 while T1 sets the state of g1=1,g2=1. As the example
        addresses, both g1=0,g2=0 and g1=1,g2=1 are legal states.
      </p>
<a name="toward_boost_stm.overview.intro.direct_and_deferred_update"></a><h5>
<a name="id4805268"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.direct_and_deferred_update">Direct
        and Deferred Update</a>
      </h5>
<p>
        Updating is the process of committing transactional writes to global memory
        and is performed in either a direct or deferred manner. Figure 6 presents
        a step-by-step analysis of direct and deferred updating.
      </p>
<p>
        Deferred update creates a local copy of global memory, performs modifications
        to the local copy, and then writes those changes to global memory if the
        transaction commits. If the transaction aborts, no additional work is done.
        Direct update makes an original backup copy of global memory and then writes
        directly to global memory. If the transaction commits, the transaction does
        nothing. If the transaction aborts, the transaction restores global memory
        with its backup copy. Some TM systems favor direct update due to its natural
        optimization of commits (BSTM, McRTSTM and LogTM). However, other TM systems
        favor deferred update due to its support for truly optimistic critical sections
        (TBoost.STM and RingSTM).
      </p>
<p>
        Direct update enables greater TM throughput when aborts are relatively low
        because it optimizes the common commit case. Deferred update enables greater
        TM throughput when
      </p>
<div class="orderedlist"><ol type="1">
<li>
          aborts are relatively high or
        </li>
<li>
          short running transactions (e.g., those that complete quickly) are executed
          alongside long running transactions (e.g., those that do not complete quickly)
          because long running transactions do not stall shorter running ones as
          they would in direct update systems, and therefore the fastest transactions
          can commit first.
        </li>
</ol></div>
<p>
        It is important to note that deferred update supports truly optimistic critical
        sections without special effort, while direct update does not. Truly optimistic
        critical sections enable the speculative execution of transactions that arrive
        after a memory location has already been tentatively written to by another
        transaction. This allows the first transaction, of potentially many competing
        transactions, to complete its commit, whether it be the later arriving transaction
        or the earlier arriving writer. This scenario is not possible with direct
        update without special effort.
      </p>
<a name="toward_boost_stm.overview.intro.scalability__mutual_exclusion_and_transactional_memory"></a><h5>
<a name="id4805358"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.scalability__mutual_exclusion_and_transactional_memory">Scalability:
        Mutual Exclusion and Transactional Memory</a>
      </h5>
<p>
        The scalability of mutual exclusion is limited to pessimistic concurrency.
        By definition, mutual exclusion's critical sections must be pessimistic,
        otherwise they would not be isolated to a single thread (i.e., they would
        not be mutually exclusive). TM, however, is generally implemented using optimistic
        concurrency, but it can enforce pessimistic concurrency amongst transactions
        if that behavior is required for certain conditions. In certain cases, TMs
        becomemore strict and execute pessimistically to enable inevitable or irrevocable
        transactions. Such transactions have significant importance for handling
        operations that, once started, must complete (e.g., I/O operations).
      </p>
<p>
        Since TM can execute optimistically and pessimistically, it is clear that
        whatever benefits pessimistic concurrency has can be acquired by TM. However,
        since mutual exclusion can only execute pessimistically, the advantages found
        in optimistic concurrency can never be obtained by mutual exclusion.
      </p>
<p>
        When one first analyzes pessimistic and optimistic concurrency, it may seem
        that the only benefit optimistic concurrency has over pessimistic concurrency
        is that multiple critical sections, which conflict on the memory they access,
        can execute concurrently. The simultaneous execution of such conflicting
        critical sections allows the execution speed of such critical sections to
        guide the system in deciding which execution should be allowed to commit
        and which should be aborted. In particular, the first process to complete
        the critical section can be allowed to abort the other process of the system.
        The same scenario cannot be achieved by pessimistic critical sections and
        is demonstrated in Section 4.1.1.
      </p>
<p>
        A counterargument to this scenario is that such optimistic concurrency only
        allows one critical section to commit, while one must be aborted. Because
        mutual exclusion only allows one conflicting critical section execution at
        a time, and because mutual exclusion does not support failure atomicity (i.e.,
        rollbacking of the critical section), mutual exclusion's pessimistic behavior
        is superior in terms of energy and efficiency. Mutual exclusion, unlike TM,
        suffers no wasted work because conflicting critical sections are limited
        to a single thread of execution, reducing the energy it uses. Furthermore,
        because mutual exclusion does not require original data to copied, as needed
        for TM's direct or deferred update, it executes faster.
      </p>
<p>
        While there is merit to this counterargument, an important scenario is not
        captured by it: truly optimistic critical sections can support multiple reader
        / single write executions which, if executed so the readers commit before
        the writer, all critical sections will succeed. This scenario is impossible
        to achieve using pessimistic critical sections. Although mutual exclusion
        can use read/write locking, as soon as a writer thread begins execution on
        a conflicting critical section, all readers must be stalled. TM's truly optimistic
        concurrency does not suffer from this overly pessimistic limitation of throughput
        and is therefore capable of producing an immeasurable amount of concurrent
        throughput under such conditions.
      </p>
<p>
        From a theoretical perspective, given L memory locations and P processes,
        mutual exclusion can support the consistent concurrent execution of P*L number
        of readers or L writers. TM can support the consistent concurrent execution
        of P*L number of readers and L writers. Using the above variables, the mathematical
        expression of the performance scalability of mutual exclusion (S(ME)) is:
      </p>
<pre class="programlisting"><span class="identifier">S</span><span class="special">(</span><span class="identifier">ME</span><span class="special">)</span> <span class="special">=</span> <span class="special">(</span><span class="identifier">P</span><span class="special">*</span><span class="identifier">L</span><span class="special">)</span> <span class="special">+</span> <span class="identifier">L</span>
</pre>
<p>
        Using the same variables, the mathematical expression of the performance
        scalability of transactional memory is:
      </p>
<pre class="programlisting"><span class="identifier">S</span><span class="special">(</span><span class="identifier">TM</span><span class="special">)</span> <span class="special">=</span> <span class="special">(</span><span class="identifier">P</span> <span class="special">*</span> <span class="identifier">L</span><span class="special">)</span> <span class="special">+</span> <span class="identifier">L</span>
</pre>
<p>
        As should be clear from the above equations, mutual exclusion cannot achieve
        the same performance scalability of TM. This is because TM supports truly
        optimistic concurrency and mutual exclusion is confined to pessimistic concurrency.
        While other examples exist that demonstrate optimistic concurrency can increase
        throughput via contention management, the above equations capture the indisputable
        mathematical limitations in mutual exclusion's performance scalability.
      </p>
<a name="toward_boost_stm.overview.intro.modularity"></a><h5>
<a name="id4858700"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.modularity">Modularity</a>
      </h5>
<p>
        Software modularity is an important aspect of software that is necessary
        for its reuse. Formally, software is modular if it can be composed in a new
        system without altering its internal implementation. Informally, software
        is modular if it can be used, in its entirety, through its interface.
      </p>
<p>
        By making software modular, it can be freely used in an unlimited number
        of software systems. Without software modularity, software can only be used
        in the original system where it was written. Clearly, without software modularity,
        software cannot be reused. Because most software developments are based on
        extensive library use, software reuse is an integral part of software development.
        As such, limiting software reuse, would result in severely hampered development
        capabilities and overall development time. For these reasons, software modularity
        is vital for any software paradigm to be practical. Software paradigms that
        do not support software modularity are, in short, impractical.
      </p>
<a name="toward_boost_stm.overview.intro.mutual_exclusion_and_software_modularity"></a><h5>
<a name="id4858744"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.mutual_exclusion_and_software_modularity">Mutual
        Exclusion and Software Modularity</a>
      </h5>
<p>
        In this section, we show that mutual exclusion, regardless of its implementation,
        fails to deliver software modularity. We demonstrate this through a running
        example started in Figure 7 which implements inc(), mult() and get(); these
        functions use lock G to respectively implement an increment, multiply and
        get operations for the shared data.
      </p>
<pre class="programlisting"><span class="number">1</span>   <span class="keyword">void</span> <span class="identifier">inc</span><span class="special">(</span><span class="keyword">int</span> <span class="identifier">v</span><span class="special">)</span> <span class="special">{</span>
<span class="number">2</span>       <span class="identifier">lock</span><span class="special">(</span><span class="identifier">G</span><span class="special">);</span> <span class="identifier">g</span> <span class="special">+=</span> <span class="identifier">v</span><span class="special">;</span> <span class="identifier">unlock</span><span class="special">(</span><span class="identifier">G</span><span class="special">);</span>
<span class="number">3</span>   <span class="special">}</span>
<span class="number">4</span>
<span class="number">5</span>   <span class="keyword">void</span> <span class="identifier">mult</span><span class="special">(</span><span class="keyword">int</span> <span class="identifier">v</span><span class="special">)</span> <span class="special">{</span>
<span class="number">6</span>       <span class="identifier">lock</span><span class="special">(</span><span class="identifier">G</span><span class="special">);</span> <span class="identifier">g</span> <span class="special">*=</span> <span class="identifier">v</span><span class="special">;</span> <span class="identifier">unlock</span><span class="special">(</span><span class="identifier">G</span><span class="special">);</span>
<span class="number">7</span>   <span class="special">}</span>
<span class="number">8</span>
<span class="number">9</span>   <span class="keyword">int</span> <span class="identifier">get</span><span class="special">()</span> <span class="special">{</span>
<span class="number">10</span>      <span class="identifier">lock</span><span class="special">(</span><span class="identifier">G</span><span class="special">);</span> <span class="keyword">int</span> <span class="identifier">v</span> <span class="special">=</span> <span class="identifier">g</span><span class="special">;</span> <span class="identifier">unlock</span><span class="special">(</span><span class="identifier">G</span><span class="special">);</span>
<span class="number">11</span>      <span class="keyword">return</span> <span class="identifier">v</span><span class="special">;</span>
<span class="number">12</span>  <span class="special">}</span>

<span class="identifier">Figure</span> <span class="number">7.</span> <span class="identifier">Mutual</span> <span class="identifier">Exclusion</span> <span class="keyword">for</span> <span class="identifier">Increment</span><span class="special">,</span> <span class="identifier">Multiply</span> <span class="keyword">and</span> <span class="identifier">Get</span> <span class="identifier">of</span> <span class="identifier">Shared</span> <span class="identifier">Variable</span><span class="special">.</span>
</pre>
<p>
        Now suppose a programmer wants to increment and multiply by some values within
        the same atomic operation. The initial implementation may look like the following.
      </p>
<pre class="programlisting"><span class="number">1</span>   <span class="identifier">inc</span><span class="special">(</span><span class="identifier">a</span><span class="special">);</span>
<span class="number">2</span>   <span class="identifier">mult</span><span class="special">(-</span><span class="identifier">b</span><span class="special">);</span>
</pre>
<p>
        An unwanted side-effect of such an implementation is the exposure of the
        intermediate state of g between inc() and mult(). A second thread performing
        a get() may read an inconsistent value of g; the value of g between inc()
        and mult(). This is demonstrated in the timing diagram of Figure 8 .
      </p>
<pre class="programlisting"><span class="identifier">Figure</span> <span class="number">8.</span> <span class="identifier">Example</span> <span class="identifier">of</span> <span class="identifier">Exposed</span> <span class="identifier">State</span> <span class="identifier">of</span> <span class="identifier">Mutual</span> <span class="identifier">Exclusion</span><span class="special">.</span>
</pre>
<p>
        If the programmer needs the inc() and mult() operations to be executed together,
        without an intermediate state being revealed, he or she could make lock G
        reentrant. Reentrant locks are locks that can be obtained multiple times
        by a single thread without deadlocking. If G is made reentrant, the following
        code could be used to make inc(a) and mult(-b) atomic. A basic implementation
        of a reentrant lock is to associate a counter with its lock and increment
        the counter each time the lock() interface is called and to decrement the
        counter each time the unlock() interface is called. The reentrant lock is
        only truly locked when a call to lock() is made when its associated counter
        is 0. Likewise, the reentrant lock is only truly unlocked when a call to
        unlock() is made when its associated counter is 1.
      </p>
<pre class="programlisting"><span class="number">1</span>   <span class="identifier">lock</span><span class="special">(</span><span class="identifier">G</span><span class="special">);</span>
<span class="number">2</span>   <span class="identifier">inc</span><span class="special">(</span><span class="identifier">a</span><span class="special">);</span>
<span class="number">3</span>   <span class="identifier">mult</span><span class="special">(-</span><span class="identifier">b</span><span class="special">);</span>
<span class="number">4</span>   <span class="identifier">unlock</span><span class="special">(</span><span class="identifier">G</span><span class="special">);</span>
</pre>
<p>
        If the above code uses reentrant locks, it will achieve the programmer's
        intended atomicity for inc() and mult(), isolating the state between inc()
        and mult(), which disallows the unwanted side-effect shown in Figure 8. While
        the atomicity of the operations is achieved, it is only achieved by exposing
        the implementation details of inc() and mult(). In particular, if the programmer
        had not known that lock G was used within inc() and mult(), making an atomic
        operation of inc() and mult() would be impossible.
      </p>
<p>
        An external atomic grouping of operations is impossible using embedded mutual
        exclusion without exposing the implementation details because the heart of
        mutual exclusion is based on named variables which the programmer specifies
        to guard their critical sections. Because these variables are named, they
        cannot be abstracted away and any programmer wishing to reuse the mutually
        exclusive code must be able to access and extend the implementation details.
      </p>
<pre class="programlisting"><span class="number">1</span>   <span class="keyword">void</span> <span class="identifier">inc</span><span class="special">(</span><span class="keyword">int</span> <span class="identifier">v</span><span class="special">)</span> <span class="special">{</span> <span class="identifier">atomic</span> <span class="special">{</span> <span class="identifier">g</span> <span class="special">+=</span> <span class="identifier">v</span><span class="special">;</span> <span class="special">}</span> <span class="special">}</span>
<span class="number">2</span>
<span class="number">3</span>   <span class="keyword">void</span> <span class="identifier">mult</span><span class="special">(</span><span class="keyword">int</span> <span class="identifier">v</span><span class="special">)</span> <span class="special">{</span> <span class="identifier">atomic</span> <span class="special">{</span> <span class="identifier">g</span> <span class="special">*=</span> <span class="identifier">v</span><span class="special">;</span> <span class="special">}</span> <span class="special">}</span>
<span class="number">4</span>
<span class="number">5</span>   <span class="keyword">int</span> <span class="identifier">get</span><span class="special">()</span> <span class="special">{</span> <span class="identifier">atomic</span> <span class="special">{</span> <span class="keyword">return</span> <span class="identifier">g</span><span class="special">;</span> <span class="special">}</span> <span class="special">}</span>

<span class="identifier">Figure</span> <span class="number">9.</span> <span class="identifier">TM</span> <span class="identifier">of</span> <span class="identifier">Increment</span><span class="special">,</span> <span class="identifier">Multiply</span> <span class="keyword">and</span> <span class="identifier">Get</span> <span class="identifier">of</span> <span class="identifier">Shared</span> <span class="identifier">Variable</span><span class="special">.</span>
</pre>
<a name="toward_boost_stm.overview.intro.summary_of_mutual_exclusion_modularity"></a><h5>
<a name="id4859757"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.summary_of_mutual_exclusion_modularity">Summary
        of Mutual Exclusion Modularity</a>
      </h5>
<p>
        As we presented at the beginning of this section, software modularity can
        be informally understood as a component's ability to be used entirely from
        its interface. Therefore, components that cannot be used entirely from their
        interface, components that must expose their implementation details to be
        extended, are not modular. As such, the paradigm of mutual exclusion does
        not support software modularity.
      </p>
<a name="toward_boost_stm.overview.intro.transactional_memory_and_software_modularity"></a><h5>
<a name="id4859797"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.transactional_memory_and_software_modularity">Transactional
        Memory and Software Modularity</a>
      </h5>
<p>
        Transactional memory works in a fundamentally different manner than mutual
        exclusion, with regard to its interface and implementation. To begin, as
        demonstrated in Section 3, TMs do not generally expose any of their implementation
        details to client code. In fact, in many TMs, client code is more versatile
        if it knows and assumes nothing about the active implementation of the TM.
        By abstracting away details of TM implementation, a TM subsystem can adapt
        its behavior to the most efficient configuration for the program's current
        workload, much like the algorithms used for efficient operation of processes
        controlled by operating systems. TM uses such abstractions to optimize the
        performance of concurrent programs using various consistency checking methods,
        conflict detection times, updating policies, and contention management schemes.
      </p>
<a name="toward_boost_stm.overview.intro.achieving_tm_software_modularity"></a><h5>
<a name="id4859838"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.achieving_tm_software_modularity">Achieving
        TM Software Modularity</a>
      </h5>
<p>
        TM achieves software modularity by allowing transactions to nest. With transactional
        nesting, individual transactions can be wrapped inside of other transactions
        which call the methods where they reside, resulting in a transaction composed
        of both the parent and child transaction. Furthermore, this is achieved without
        altering or understanding the child transaction's implementation. To best
        demonstrate transactional nesting, we reuse the prior mutual exclusion example
        shown in Figure 7 and implement it using transactions as shown in Figure
        9.
      </p>
<p>
        As before, the programmer's goal is to implement a combination of inc() and
        mult() executed in an atomic fashion. The basic, and incorrect implementation
        is demonstrated below:
      </p>
<pre class="programlisting"><span class="number">1</span>   <span class="identifier">inc</span><span class="special">(</span><span class="identifier">a</span><span class="special">);</span>
<span class="number">2</span>   <span class="identifier">mult</span><span class="special">(-</span><span class="identifier">b</span><span class="special">);</span>
</pre>
<p>
        Even with transactions, this approach fails because the transactions within
        inc() and mult() begin and end inside their respective functions. However,
        to make the above operations atomic, the programmer need only make the following
        modification shown in Figure 10.
      </p>
<pre class="programlisting"><span class="number">1</span>   <span class="identifier">atomic</span> <span class="special">{</span> <span class="comment">// atomic {
</span><span class="number">2</span>       <span class="identifier">inc</span><span class="special">(</span><span class="identifier">a</span><span class="special">);</span> <span class="comment">// atomic { g += a; }
</span><span class="number">3</span>       <span class="identifier">mult</span><span class="special">(-</span><span class="identifier">b</span><span class="special">);</span> <span class="comment">// atomic { g *= -b; }
</span><span class="number">4</span>   <span class="special">}</span> <span class="comment">// }
</span>
<span class="identifier">Figure</span> <span class="number">10.</span> <span class="identifier">Modularity</span><span class="special">:</span> <span class="identifier">Transaction</span> <span class="identifier">of</span> <span class="identifier">Increment</span> <span class="keyword">and</span> <span class="identifier">Multiply</span><span class="special">.</span>
</pre>
<p>
        In effect, the TM system subsumes the transactions that are nested inside
        of the inc() and mult() operations. 6 The left side of Figure 10 shows the
        actual code of the transaction, while the right side shows the child transactions
        that are subsumed by the parent transaction.
      </p>
<p>
        Because transactions are isolated and atomic, the resulting state of g, from
        operations inc() and mult(), is invisible to outside observers until the
        transaction is committed. As such, outside threads cannot view any intermediate
        state constructed by partial transaction execution. The result of such isolated
        behavior is the guaranteed consistent concurrent execution of interleaved
        accesses to shared memory from in-flight transactions. This is demonstrated
        in Figure 11; let g=0 and assume deferred update is the active updating policy,
        as explained in Section 4.2.
      </p>
<pre class="programlisting"><span class="identifier">Figure</span> <span class="number">11.</span> <span class="identifier">Example</span> <span class="identifier">of</span> <span class="identifier">Isolated</span> <span class="keyword">and</span> <span class="identifier">Consistent</span> <span class="identifier">State</span> <span class="identifier">of</span> <span class="identifier">TM</span><span class="special">.</span>
</pre>
<p>
        As shown in Figure 11, multiple concurrently executing threads can read and
        write to the same shared memory in a consistent and isolated fashion when
        using TM. In the example, thread T2 performs x = get() after T1 has already
        executed inc(a). However, since T1 has not yet committed its transaction,
        T2's view of shared data g is consistent (g=0). When T2 begins the commit
        phase of its transaction, the TM subsystem verifies that shared data g has
        not been updated since it initially read it. Since no other transaction has
        updated shared data g, T2's transaction is permitted to commit. Thread T1
        then continues with its mult() operation and then enters its commit phase.
        The TM subsystem also verifies the consistency of T1's transaction before
        it is allowed to commit. Again, since no one transaction has updated shared
        data g between its reads and writes to it, T1's transaction is permitted
        to commit.
      </p>
<p>
        The above analysis demonstrates that software modularity can be achieved
        in TM through transactional nesting (Figure 10). In this case, the specific
        software modularity achieved is extension to an existing critical section.
        Critical section extension was also possible with mutual exclusion, as demonstrated
        in Section 5.1, but only through exposing the details behind the mutual exclusion
        implementation. Due to this, mutual exclusion fails to deliver a practical
        level of software modularity.
      </p>
<a name="toward_boost_stm.overview.intro.summary_of_transactional_memory_modularity"></a><h5>
<a name="id4860198"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.summary_of_transactional_memory_modularity">Summary
        of Transactional Memory Modularity</a>
      </h5>
<p>
        TM supports software modularity by allowing transactions to nest, to any
        depth, while logically grouping the shared data accesses within the transactions
        into an atomic, consistent and isolated (ACI) operation. Transactional nesting
        is natural to the programmer because nested transactions behave in the same
        manner as unnested transactions. TM's ACI support ensures transactions will
        behave in a correct manner regardless of if the transaction is used by itself
        or subsumed into a larger transaction.
      </p>
<a name="toward_boost_stm.overview.intro.c___library_language_like_solution"></a><h5>
<a name="id4860232"></a>
        <a class="link" href="intro.html#toward_boost_stm.overview.intro.c___library_language_like_solution">C++
        library language-like solution</a>
      </h5>
<p>
        Research in parallel programming has recently seen a flurry of attention.
        Among the active research is a push for high-level languages to offer native
        support for parallel programming primitives. The next version of C++ will
        incorporate library support for threads, while numerous researchers are exploring
        ways to extend C++ to support transactional memory (TM).
      </p>
<p>
        A strength of C++ is its support for automatic objects. Rather than requiring
        that parallel primitives be added directly to the language, automatic objects
        in C++ can be used to implement much of their necessary infrastructure. The
        automatic object approach is natural from a language perspective, provides
        full algorithmic control to the end programmer, and demonstrates C++'s linguistic
        elegance. The disadvantage of this approach is its added programmatic overhead.
        Using only automatic objects, certain programming errors, such as accidental
        scope removal and incorrectly programmed transactional retry behavior, can
        arise.
      </p>
<p>
        In light of this, there are unique trade-offs between language based and
        library-based parallel primitives. Language-based solutions minimize syntactic
        clutter which reduce programmer related errors, but are seemingly irreversible
        and, if incorrect, can have crippling effects upon the language. Library-based
        solutions increase programmer control and flexibility, but place substantial
        pressure on the programmer to avoid minute programming errors. A good compromise
        is a solution that behaves like a language extension, but is implemented
        within a library. By implementing parallel primitives within a library that
        uses language-like interfaces, programmer pressure is reduced, implementation
        updates are seamless, and full programmer control is achieved through library
        extensibility.
      </p>
<p>
        TBoost.STM present such a language-like solution for C++ using generic library
        coupled with a deliberate use of the preprocessor. The culmination of these
        components facilitate a simple, yet powerful, parallel programming interface
        in C++.
      </p>
</div>
<table xmlns:rev="http://www.cs.rpi.edu/~gregod/boost/tools/doc/revision" width="100%"><tr>
<td align="left"></td>
<td align="right"><div class="copyright-footer">Copyright © 2009 Justin E. Gottchlich<br>Copyright © 2009 Vicente J. Botet Escriba<p>
        Distributed under the Boost Software License, Version 1.0. (See accompanying
        file LICENSE_1_0.txt or copy at <a href="http://www.boost.org/LICENSE_1_0.txt" target="_top">http://www.boost.org/LICENSE_1_0.txt</a>)
      </p>
</div></td>
</tr></table>
<hr>
<div class="spirit-nav">
<a accesskey="p" href="../overview.html"><img src="../../../../../../doc/html/images/prev.png" alt="Prev"></a><a accesskey="u" href="../overview.html"><img src="../../../../../../doc/html/images/up.png" alt="Up"></a><a accesskey="h" href="../../index.html"><img src="../../../../../../doc/html/images/home.png" alt="Home"></a><a accesskey="n" href="../users_guide.html"><img src="../../../../../../doc/html/images/next.png" alt="Next"></a>
</div>
</body>
</html>
