[library Unicode
    [quickbook 1.3]
    [version 0.1 preview 3]
    [authors [Gaunard, Mathias]]
    [copyright 2009 Mathias Gaunard]
    [category string-text]
    [purpose Internationalized text handling in C++ with Unicode]
    [license
        Distributed under the Boost Software License, Version 1.0.
        (See accompanying file LICENSE_1_0.txt or copy at
        [@http://www.boost.org/LICENSE_1_0.txt])
    ]
]

[/ Some links]

[def __note__           [$images/note.png]]
[def __alert__          [$images/alert.png]]
[def __tip__            [$images/tip.png]]

[def __unicode_std__    [@http://www.unicode.org/versions/latest/ Unicode Standard]]
[def __tr10__           [@http://unicode.org/reports/tr10/ Technical Standard #10 - Unicode Collation Algorithm]]
[def __tr15__           [@http://unicode.org/reports/tr15/ Annex #15 - Normalization Forms]]
[def __tr29__           [@http://unicode.org/reports/tr29/ Annex #29 - Text Segmentation]]
[def __boost_range__    [@http://boost.org/libs/range/index.html Boost.Range]]

[section Preface]

[:Unicode is the industry standard to consistently represent and manipulate text across most of the world's writing systems.]


[heading Description]

This library aims at providing the foundation tools to accurately represent and deal with natural text in C++ in a portable
and robust manner, so as to allow internationalized applications, by implementing parts of the __unicode_std__.

This library is environment-independent and deliberately chooses not to relate to the standard C++ locale facilities
as well as the standard string facilities, judged ill-suited to Unicode.

The current version is locale-agnostic, but a subsystem for tailored locale behaviour may be added in the future.

[warning Boost.Unicode is a library in development and is not part of Boost.]

[heading How to use this manual]

Some icons are used to mark certain topics indicative of their relevance. These
icons precede some text to indicate:

[table Icons
    [[Icon]         [Name]          [Meaning]]
    [[__note__]     [Note]          [Information provided is auxiliary but will
                                     give the reader a deeper insight into a specific
                                     topic. May be skipped.]]
    [[__alert__]    [Alert]         [Information provided is of utmost importance.]]
    [[__tip__]      [Tip]           [A potentially useful and helpful piece of
                                     information.]]
]

[endsect]

[section Introduction to Unicode]

[section Character set]
The Unicode character set is a mapping that associates *code points*, which are integers, to characters for any writing system or language.

As of version 5.1, there are 100,507 characters, requiring a storage capacity of 17 bits per code point. The unicode standard however
also reserves some code ranges, known as planes, meaning it really requires a storage capacity of 21 bits.

Since microprocessors usually deal with integers whose capacity are multiples of 8 bits, a naive usage would be to use 32 bits per code point,
which is quite a waste, especially since most daily-used characters lie in the Basic Multilingual Plane, which fits on 16 bits.

That is why variable-width encodings were designed, where each code point is represented by a variable number of *code units*, formely also known as code values.
[endsect]

[section Encodings]

The UTF-X family of encodings encode a single *code point* into a variable number of *code units*, each of which does X bits.

[heading UTF-32]

This encoding is fixed-width, each code unit is simply a code point.

This encoding isn't really recommended for internal representations other
that for use with algorithms that strictly require random access of code points.

[heading UTF-16]

Every code point is encoded by one or two code units. If the code point lies within the BMP, it is represented by exactly that code point.
Otherwise, the code point is represented by two units which both lie in the surrogate category of Unicode code points.

This is the recommended encoding for dealing with Unicode internally for general purposes, since it has fairly low processing overhead
compared to UTF-8 and doesn't waste as much memory as UTF-32.

[heading UTF-8]

This encoding was designed to be compatible with legacy, 8-bit based, text management.

Every code point within ASCII is represented as exactly that ASCII character, others are represented as a variable-sized sequence from
two to four bytes, all of which are non-ASCII.

This encoding is popular for data storage and interchange, but can also be useful for compatibility with byte-oriented string manipulation.

[endsect]

[section Combining character characters]

A *non-combining code point* may be followed by an arbitrary number of *combining code points* to form a single *combining character sequence*, which is really a composite character.

Certain characters are only available as a combination of multiple code points, while some, the ones that are expected to be the most used,
are also available as a single precomposed code point. The order of the combined code points may also vary, but all code points combinations
leading to the same character are still canonically equivalent.

While a combining character sequence can be arbitrarily big, the Unicode standard also introduces the concept of a *stream-safe string*, where
a it is at most 31 code points long, which is largely sufficient for any real use.

[endsect]

[section Grapheme clusters]

Another yet higher-level abstraction of character is that of a *grapheme cluster*, which really corresponds to what a human would call a character.
All *combining character sequences* are graphemes, but there are other sequences of *code points* that are as well; for example =\r\n= is one.

For certain classes of applications, such as word processors, it is important to operate at the *grapheme* level rather than at the *code point* or
*combining character sequence* one.

[endsect]

[section Normalization]

The Unicode standard defines four normalized forms in __tr15__ where *combining character sequences* are either fully compressed or decompressed,
using either canonical or compatiblity decompositions.

The Normalized Form C is of a great interest, as it compresses every grapheme so that is uses as few code points as possible. It's also
the one that operates best with legacy systems unaware of combining character sequences, font rendering systems and is also
the normalized form assumed by the XML standard.

On the other hand, the Normalized Form D uses a lot more space, but is more efficient to compute and to use
when concatenating combining characters to a string while maintaining the form.
[endsect]

[section Other operations]
The Unicode standard also specifies various features such as a collation algorithm in __tr10__ for comparison and ordering of strings with
a locale-specific criterion, as well as mechanisms to iterate over words, sentences and lines in __tr29__.

Those features are not implemented by the current version of the library.
[endsect]

[section Character properties]

Unicode also provides a database of character properties called the Unicode Character Database (UCD), which consists of a set of files describing
the following properties:

* Name.
* General category (classification as letters, numbers, symbols, punctuation, etc.).
* Other important general characteristics (white space, dash, ideographic, alphabetic, non character, deprecated, etc.).
* Character shaping (bidi category, shaping, mirroring, width, etc.).
* Case (upper, lower, title, folding; both simple and full).
* Numeric values and types (for digits).
* Script and block.
* Normalization properties (decompositions, decomposition type, canonical combining class, composition exclusions, etc.).
* Age (version of the standard in which the code point was first designated).
* Boundaries (grapheme cluster, word, line and sentence).
* Standardized variants.

The database is useful for Unicode implementation in general, as it is the base for most algorithms, but can also be of interest to the library user that wants to
implement facilities not provided by the library core.

[endsect]
[endsect]

[section Linking the library]

As has been stated in [link unicode.introduction_to_unicode.character_properties Introduction to Unicode], several Unicode algorithms require the usage of a large
database of information which, as of the preview 3 of this library, is 600 KB on x86. Note the database does not contain everything one might need at this
stage of the development of the library.

Features that can avoid dependency on that database do so; so it is not required for UTF conversions for example, that are purely header-only.

[heading UCD generation]

The Unicode Character Database can be generated using a parser present in the source distribution of this library to analyze
[@http://www.unicode.org/Public/ the data provided by Unicode.org].

Note however that the parser itself needs to be updated to be made aware of new proprieties; otherwise those properties will fallback to the default value
and the parser will issue a warning.

[heading Binary compatibility]

This library does not provide any kind of binary compatibility of the UCD so that applications compiled with version X of the library may actually
link to version Y of the libray, with Y >= X, partially due to performance considerations.

This may change in the future once proper benchmarking has been done.

[heading Alternate databases]

Future versions of this library may provide alternate implementations of this database as a thin layer over a database provided by another library or environment
to prevent duplication of data.

[endsect]

[section Overview]

[section Range operations]

This library provides two kinds of operations on bidirectional ranges:
conversion (e.g. converting a range in UTF-8 to a range in UTF-32) and
segmentation (i.e. demarcating sections of a range, like code points,
grapheme clusters, words, etc.).

[heading Conversion]
Conversions can be applied in a variety of means, all generated from using
the [conceptref Pipe] concept that performs one step of the conversion:

* Eager evaluation, with simply
loops the =Pipe= until the whole input range has been treated.
* Lazy evaluation, where a new range is returned that wraps the input range
and converts step-by-step as the range is advanced. The resulting range is
however read-only. It is implemented in terms of [classref boost::pipe_iterator].
* Lazy output evaluation, where an output iterator is returned that wraps the output
and converts every pushed element with a [conceptref OneManyPipe]. It is implemented in terms
of [classref boost::pipe_output_iterator].

The naming scheme of the utilities within the library reflect this; here is
for example what is provided to convert UTF-32 to UTF-8:

* [classref boost::unicode::u8_encoder] is a model of the =OneManyPipe= concept.
* [funcref boost::unicode::u8_encode] is an eager encoding algorithm.
* [funcref boost::unicode::u8_encoded] returns a range adapter that does on-the-fly encoding.
* [funcref boost::unicode::u8_encoded_out] returns an output iterator adapter that will encode its elements before forwarding them to the wrapped output iterator.

[note The library considers a conversion from UTF-32 an "encoding", while a conversion
to UTF-32 is called a "decoding".
This is because code points is what the library mainly deals with, and UTF-32 is a sequence of code points.]

[heading Segmentation]
Segmentations are expressed in terms of the [conceptref Consumer] concept, which is inherently
very similar to the [conceptref Pipe] concept excepts it doesn't perform any kind of transformation,
it just reads part of the input.
As a matter of fact, a =Pipe= can be converted to =Consumer= using [classref boost::pipe_consumer].

Segmentation may be done either by using the appropriate =Consumer= directly, or by using the
[classref boost::consumer_iterator] template to adapt the range into a
read-only range of subranges.

Additionally, the [conceptref BoundaryChecker] concept may prove useful to tell whether
a segment starts at a given position; a =Consumer= may also be defined
in terms of it using [classref boost::boundary_consumer].

The naming scheme is as follows:

* [classref boost::unicode::u8_boundary] is a =BoundaryChecker= that tells whether a position is the start of a code point in a range of UTF-8 code units.
* [classref boost::unicode::grapheme_boundary] is a =BoundaryChecker= that tells whether a position is the start of a grapheme cluster in a range of code points.
* [funcref boost::unicode::u8_bounded] adapts its input range in UTF-8 into a range of ranges of code units, each range being a code point.
* [funcref boost::unicode::grapheme_bounded] adapts its input range in UTF-32 into a range of ranges of code points, each range being a grapheme cluster.
* [funcref boost::unicode::u8_grapheme_bounded] adapts its input range in UTF-8 into a range of code units, each range being a grapheme cluster.

[heading UTF type deduction with SFINAE]
Everytime there are two versions for a function or class, one for UTF-8 and
the other for UTF-16, and deducing which type of UTF encoding to use is
possible, additional ones are added that will automatically forward to it.

The naming scheme is as follows:

* [funcref boost::unicode::utf_decode] either behaves like [funcref boost::unicode::u8_decode], [funcref boost::unicode::u16_decode]
depending on the =value_type= of its input range.
* [classref boost::unicode::utf_boundary] either behaves like
[classref boost::unicode::u8_boundary] or [classref boost::unicode::u16_boundary]
depending on the =value_type= of the input ranges passed to =ltr= and =rtl=.

[tip Not only UTF-8 and UTF-16 are recognized by UTF type deduction, UTF-32 is as well.]

[endsect] [/Range operations]

[section Composition and Normalization]

Normalized forms are defined in terms of certain decompositions applied
recursively, followed by certain compositions also applied recursively,
and finally canonical ordering of combining character sequences.

A decomposition being a conversion of a single code point into several
and a composition being the opposite conversion, with exceptions.

[heading Decomposition]
The Unicode Character Database associates with code points certain decompositions,
which can be obtained with [funcref boost::unicode::ucd::get_decomposition],
but does not include Hangul syllable decompositions since those can be easily
procedurally generated, allowing space to be saved.

The library provides [classref boost::unicode::hangul_decomposer], a
[conceptref OneManyPipe] to decompose Hangul syllables.

There are several types of decompositions, which are exposed by
[funcref boost::unicode::ucd::get_decomposition_type], most importantly
the canonical composition is obtained by applying both the Hangul
decompositions and the canonical decompositions from the UCD, while the
compatibility decomposition is obtained by applying the Hangul decompositions
and all decompositions from the UCD.

[classref boost::unicode::decomposer], model of [conceptref Pipe]
allows to perform any decomposition that matches a certain mask, recursively,
including Hangul ones (which are treated as canonical decompositions),
and canonically orders combining sequences as well.

[heading Composition]
Likewise, Hangul syllable compositions are not provided by the UCD and
are implemented by [classref boost::unicode::hangul_composer] instead.

Some distinct code points may have the same decomposition, so certain
decomposed forms are preferred. That is why an exclusion table is also
provided by the UCD.

The library uses a pre-generated prefix tree (or, in the current
implementation, a lexicographically sorted array) of all canonical
compositions from their fully decomposed and canonically ordered form to
identity composable sequences and apply the compositions.

[classref boost::unicode::composer] is a [conceptref Pipe] that uses that
tree as well as the Hangul compositions.

[heading Normalization]
Normalization can be performed by applying decomposition followed by
composition, which is what the current version of [classref boost::unicode::normalizer]
does.

The Unicode standard however provides as well quick-check properties to
avoid that operation when possible, but the current version of the library
does not support that scheme at the moment.

[heading Concatenation]
Concatenating strings in a given normalization form does not guarantee the result
is in that same normalization form if the right operand starts with a combining
code point.

Therefore the library provides functionality to identity the boundaries where
re-normalization needs to occur as well as eager and lazy versions of the
concatenation that maintain the input normalization.

Note concatenation with Normalization Form D is slightly more efficient as it only
requires canonical sorting of the combining character sequence placed at
the intersection.

See:

* [funcref boost::unicode::cat_limits] to partition into the different sub ranges.
* [funcref boost::unicode::composed_concat], eager version with input in Normalization Form C.
* [funcref boost::unicode::composed_concated], lazy version with input in Normalization Form C.
* [funcref boost::unicode::decomposed_concat], eager version with input in Normalization Form D.
* [funcref boost::unicode::decomposed_concated], lazy version with input in Normalization Form D.

[endsect] [/Normalization]

[section String searching algorithms]
The library provides mechanisms to perform searches at the code unit, code point,
or grapheme level, and in the future will provide word and sentence level
as well.

Different approaches to do that are possible:

* [conceptref Pipe]- or [conceptref Consumer]-based, you may simply run classic search algorithms, such as
the ones from Boost.StringAlgo, with ranges of the appropriate elements, that elements can be ranges
themselves (subranges returned by [classref boost::consumer_iterator] are =EqualityComparable=).
* [conceptref BoundaryChecker]-based, the classic algorithms are run, then false positives
that don't lie on the right boundaries are discarded. This has the advantage of reducing conversion and
iteration overhead in certain situations.
The most practical way to achieve this is to adapt a =Finder= in Boost.StringAlgo with [classref boost::algorithm::boundary_finder].

[important You will have to normalize input before the search if you want canonically equivalent things
to compare equal.]

[endsect] [/String searching]

[endsect] [/Overview]

[section User's Guide]
[endsect] [/User's Guide]

[section Examples]

[section convert]
[import ../example/convert.cpp]
[convert]
[endsect]

[section characters]
[import ../example/characters.cpp]
[characters]
[endsect]

[section compose]
[import ../example/compose.cpp]
[compose]
[endsect]

[section search]
[import ../example/search.cpp]
[search]
[endsect]

[section source_input]
[import ../example/source_input.cpp]
[source_input]
[endsect]

[endsect]

[xinclude autodoc1c.xml]
[xinclude autodoc2.xml]

[section Appendices]

[section:appendix_source Appendix A: Unicode in source files]

It is often quite useful to embed strings of text directly into source
files, and C++, as of the 2003 standard, provides the following ways to
do so: string literals, wide string literals, character and
wide character literals, and finally type lists that form
compile-time strings. One has to be aware, however, of the various
portability issues associated with character encoding within source
files.

The first limitation is that of what character encoding the compiler
expects the source files to be in, the source character set. The second
one is what character encoding narrow and wide string literals will
have at runtime: the execution character set, which may be different for
narrow and wide strings.

Indeed, while certain compilers remain encoding-agnostic as long as the
source is ASCII-compatible, certain will convert the string literals
from the source character set to their appropriate execution character
sets.
This is the case of MSVC which, when it detects a source file is in
UTF-8 or UTF-16, will convert narrow string literals to ANSI and wide
string literals to UTF-16. Furthermore, if it doesn't detect the
character encoding of the source file, it will still convert wide string
literals from ANSI to UTF-16 while leaving narrow ones untouched.

Also, regardless of whether the compiler detects the character
encoding of the source file or not, Unicode escape sequences,
[^\u['xxxx]] and [^\U['xxxxxxxx]], will be translated to the execution
character set of the literal type they're embedded in.
Unfortunately, that makes them unusable portably within narrow strings,
as there is no way to set the narrow execution character set to UTF-8
with MSVC, and UTF-8 is the way Unices are going.

Finally, wide characters are not well defined. In practice, they're
either UTF-16 or UTF-32 code units, but their use is quite discouraged
as the size of =wchar_t= is very variable: 16 bits on MS Windows,
usually 32 bits on Unices.
Nevertheless, in the lack of UTF-16 and UTF-32 literals that are coming
with C++0x, wide string literals are probably the closest thing there is
to native Unicode in the compiler. The library tools that automatically
deduce the UTF encoding based on the size of the value
type will therefore work as expected as they will expect =wchar_t= to
represent either UTF-16 or UTF-32 depending on its size.

Alternatively, compile-time strings may be used, which allow a great
deal of flexibility as arbitrary character encoding conversion may
then be performed at compile-time, but which remain more verbose to
declare and increase compilation times.

We can then infer certain guidelines to write Unicode data within
C++ source files in a portable way while taking a few reasonable
assumptions.

[heading Portability guidelines]

* Source file encoding: use UTF-8 without a Byte Order Mark or use
ASCII. This ensures most compilers will run in an encoding-agnostic mode
and not perform translations, plus most compilers only support
ASCII-compatible input.
* Narrow character literals: use ASCII only; if you use escape sequences
such as =\x= treat the data you're inputting as UTF-8 code units. Ban
=\u= and =\U=.
* Narrow string literals: freely input UTF-8; if you use escape
sequences such as =\x= treat the data you're inputting as UTF-8 code
units. Ban =\u= and =\U=.
* Wide character literals: use ASCII only; if you use escape sequences
such as =\x= treat the data you're inputting as UTF-32 code units,
but don't input anything higher than =0xD800=. Use heavily =\u= but ban
=\U=.
* Wide string literals: use ASCII only; if you use escape sequences
such as =\x= treat the data you're inputting as UTF-32 code units,
but don't input anything higher than =0xD800=. Use heavily both =\u= and
=\U=.

[heading Compile-time strings]

Option one is to use =boost::mpl::string= as a UTF-8 compile-time
string. Its support for multi-char character literals allows it to not
be too verbose, and it can be coupled with [classref boost::unicode::string_cp]
to insert Unicode code points in stead of the Unicode escape sequences.
Any non-ASCII character shall be put as its own character literal. Note
multi-char character literals require =int= to be at least 32 bits
however.

A second option is to use =boost::mpl::u32string= as a UTF-32 compile-time
string, and use [classref boost::unicode::static_u8_encode] or
[classref boost::unicode::static_u16_encode] to eventually encode it
at compile-time to UTF-8 or UTF-16. =boost::mpl::u16string= may also be
used to directly input UTF-16.
However, none of these two sequence types provide support for easier
declaration with multi-char character literals.

Then, the =boost::mpl::c_str= meta-function may be used to convert
any compile-time string into a zero-terminated equivalent.

[heading Example]

See the [link unicode.examples.source_input source_input] example for
demonstrations.

[endsect] [/Unicode in source]

[section Appendix B: Rationale]

[heading Pipe concept]
Centralizing conversion into a single [conceptref Pipe] model allows
eager and lazy variants of evaluation to be possible for any conversion
facility.

Lazy evaluation is believed to be of great interest since it avoids the
need for memory allocations and buffers and constructing a logic
conversion is constant-time instead of linear-time since there is no need
to actually walk the range.

Eager evaluations can remain more efficient however, and that is why they
are provided as well.

[endsect] [/Rationale]

[section Appendix C: Future Work]

[heading Non-checked UTF conversion]
The library only provides UTF conversion pipes that do extensive checking
that the input is correct and that the end is not unexpectedly met.

These could be avoided when it is known that the input is valid, and
thus performance be increased. [classref boost::pipe_iterator] could as
well avoid storing the =begin= and =end= iterator in such cases.

[heading Fast Normalization]
The Unicode standard provides a quick-check scheme to tell whether a string
is in a normalized form, which could be used to avoid expensive decomposition
and recomposition.

[heading Unicode String type]
Future versions of the library could provide a string type that maintains
the following invariants: valid UTF, stream-safe and in Normalization Form C.

[endsect] [/Future Work]

[section Appendix D: Acknowledgements]

I would like to thank Eric Niebler for mentoring this project
as part of the Google Summer of Code program, who provided steady help
and insightful ideas along the development of this project.

Graham Barnett and Rogier van Dalen deserve great thanks as well for
their work on Unicode character properties, most of the parser of
Unicode data was written by them.

John Maddock was also a great help by contributing preliminary on-the-fly UTF conversion
which helped the library get started, while inspiration from Phil Endecott
allowed UTF conversion code to be more efficient.

Finally, I thank Beman Dawes and other members of the mailing list for
their interest and support.

[endsect] [/Acknowledgements]

[endsect] [/Appendices]
