[library Unicode
    [quickbook 1.3]
    [version 0.1 preview 3]
    [authors [Gaunard, Mathias]]
    [copyright 2009 Mathias Gaunard]
    [category string-text]
    [purpose Internationalized text handling in C++ with Unicode]
    [license
        Distributed under the Boost Software License, Version 1.0.
        (See accompanying file LICENSE_1_0.txt or copy at
        [@http://www.boost.org/LICENSE_1_0.txt])
    ]
]

[/ Some links]

[def __note__           [$images/note.png]]
[def __alert__          [$images/alert.png]]
[def __tip__            [$images/tip.png]]

[def __unicode_std__    [@http://www.unicode.org/versions/latest/ Unicode Standard]]
[def __tr10__           [@http://unicode.org/reports/tr10/ Technical Standard #10 - Unicode Collation Algorithm]]
[def __tr15__           [@http://unicode.org/reports/tr15/ Annex #15 - Normalization Forms]]
[def __tr29__           [@http://unicode.org/reports/tr29/ Annex #29 - Text Segmentation]]
[def __boost_range__    [@http://boost.org/libs/range/index.html Boost.Range]]

[section Preface]

[:Unicode is the industry standard to consistently represent and manipulate text across most of the world's writing systems.]


[heading Description]

This library aims at providing the foundation tools to accurately represent and deal with natural text in C++ in a portable
and robust manner, so as to allow internationalized applications, by implementing parts of the __unicode_std__.

This library is environment-independent and deliberately chooses not to relate to the standard C++ locale facilities
as well as the standard string facilities, judged ill-suited to Unicode.

The current version is locale-agnostic, but a subsystem for tailored locale behaviour may be added in the future.

[heading How to use this manual]

Some icons are used to mark certain topics indicative of their relevance. These
icons precede some text to indicate:

[table Icons
    [[Icon]         [Name]          [Meaning]]
    [[__note__]     [Note]          [Information provided is auxiliary but will
                                     give the reader a deeper insight into a specific
                                     topic. May be skipped.]]
    [[__alert__]    [Alert]         [Information provided is of utmost importance.]]
    [[__tip__]      [Tip]           [A potentially useful and helpful piece of
                                     information.]]
]

[endsect]

[section Introduction to Unicode]

[section Character set]
The Unicode character set is a mapping that associates *code points*, which are integers, to characters for any writing system or language.

As of version 5.1, there are 100,507 characters, requiring a storage capacity of 17 bits per code point. The unicode standard however
also reserves some code ranges, known as planes, meaning it really requires a storage capacity of 21 bits.

Since microprocessors usually deal with integers whose capacity are multiples of 8 bits, a naive usage would be to use 32 bits per code point,
which is quite a waste, especially since most daily-used characters lie in the Basic Multilingual Plane, which fits on 16 bits.

That is why variable-width encodings were designed, where each code point is represented by a variable number of *code units*, formely also known as code values.
[endsect]

[section Encodings]

The UTF-X family of encodings encode a single *code point* into a variable number of *code units*, each of which does X bits.

[heading UTF-32]

This encoding is fixed-width, each code unit is simply a code point.

This encoding isn't really recommended

[heading UTF-16]

Every code point is encoded by one or two code units. If the code point lies within the BMP, it is represented by exactly that code point.
Otherwise, the code point is represented by two values which both lie in the surrogate category of Unicode code points.

This is the recommended encoding for dealing with Unicode internally for general purposes, since it has fairly low processing overhead
compared to UTF-8 and doesn't waste as much memory as UTF-32.

[heading UTF-8]

This encoding was designed to be compatible with legacy, 8-bit based, text management.

Every code point within ASCII is represented as exactly that ASCII character, others are represented as a variable-sized sequence from
two to four bytes, all of which are non-ASCII.

This encoding is popular for data storage and interchange, but can also be useful for compatibility with byte-oriented string manipulation.

[endsect]

[section Composite characters]

Multiple *code points* may be combined to form a single *grapheme cluster*, which corresponds to what a human would call a character.

Certain graphemes are only available as a combination of multiple code points, while some, the ones that are expected to be the most used,
are also available as a single precomposed code point. The order of the combined code points may also vary, but all code points combinations
leading to the same grapheme are still canonically equivalent.

It is thus important to be able to apply algorithms with graphemes as the unit rather than code points to deal with graphemes not representable
by a single code point.

[endsect]

[section Normalization]

The Unicode standard defines four normalized forms in __tr15__ where *grapheme clusters* are either fully compressed or decompressed,
using either canonical or compatiblity equivalence.

The Normalized Form C is of a great interest, as it compresses every grapheme so that is uses as few code points as possible. It is also
the normalized form assumed by the XML standard.
[endsect]

[section Other operations]
The Unicode standard also specifies various features such as a collation algorithm in __tr10__ for comparison and ordering of strings with
a locale-specific criterion, as well as mechanisms to iterate over words, sentences and lines in __tr29__.

Those features are not implemented by the current version of the library.
[endsect]

[section Character properties]

Unicode also provides a database of character properties called the Unicode Character Database (UCD), which consists of a set of files describing
the following properties:

* Name.
* General category (classification as letters, numbers, symbols, punctuation, etc.).
* Other important general characteristics (white space, dash, ideographic, alphabetic, non character, deprecated, etc.).
* Character shaping (bidi category, shaping, mirroring, width, etc.).
* Case (upper, lower, title, folding; both simple and full).
* Numeric values and types (for digits).
* Script and block.
* Normalization properties (decompositions, decomposition type, canonical combining class, composition exclusions, etc.).
* Age (version of the standard in which the code point was first designated).
* Boundaries (grapheme cluster, word, line and sentence).
* Standardized variants.

The database is useful for Unicode implementation in general, as it is the base for most algorithms, but can also be of interest to the library user that wants to
implement facilities not provided by the library core.

[endsect]
[endsect]

[section Linking the library]

As has been stated in [link unicode.introduction_to_unicode.character_properties Introduction to Unicode], several Unicode algorithms require the usage of a large
database of information which, as of version 0.1 of this library, is 2.6 MB big on x86 for the "big" version including character names, arabic shaping
and all the information required for sorting and collations.
The default version which doesn't include those features is however only 520 KB.

For this reason, features that can avoid dependency on that database do so; it is not required for conversions for example. All algorithms that depend on the Unicode
Character Database are documented as such or lie in the =boost::unicode::ucd= namespace. All other features are also header-only.

[heading UCD generation]

The Unicode Character Database can be generated using a parser present in the source distribution of this library to analyze
[@http://www.unicode.org/Public/ the data provided by Unicode.org].

Note however that the parser itself needs to be updated to be made aware of new proprieties; otherwise those properties will fallback to the default value
and the parser will issue a warning.

[heading Binary compatibility]

This library does not provide any kind of binary compatibility of the UCD so that applications compiled with version X of the library may actually
link to version Y of the libray, with Y >= X, partially due to performance considerations.

This may change in the future once proper benchmarking has been done.

[heading Alternate databases]

Future versions of this library may provide alternate implementations of this database as a thin layer over a database provided by another library or environment
to prevent duplication of data.

[endsect]

[section Overview]

[section Range operations]

This library provides two kinds of operations on bidirectional ranges:
conversion (e.g. converting a range in UTF-8 to a range in UTF-32) and
segmentation (i.e. demarcating sections of a range, like code points,
grapheme clusters, words, etc.).

[heading Conversion]

Conversions can be applied in a variety of means, all generated from using
the [conceptref Pipe] and [conceptref OneManyPipe] concepts that perform one step of the conversion:

* Eager evaluation, with simply
loops the =Pipe= until the whole input range has been treated.
* Lazy evaluation, where a new range is returned that wraps the input range
and converts step-by-step as the range is advanced. The resulting range is
however read-only. It is implemented in terms of [classref boost::pipe_iterator].
* Lazy output evaluation, where an output iterator is returned that wraps the output
and converts every pushed element with a =OneManyPipe=. It is implemented in terms
of [classref boost::pipe_output_iterator].

The naming scheme of the utilities within the library reflect this; here is
for example what is provided to convert UTF-32 to UTF-8:

* [classref boost::unicode::u8_encoder] is a model of the =OneManyPipe= concept.
* [funcref boost::unicode::u8_encode] is an eager encoding algorithm.
* [funcref boost::unicode::u8_encoded] returns a range adapter that does on-the-fly encoding.
* [funcref boost::unicode::u8_encoded_out] returns an output iterator adapter that will encode its elements before forwarding them to the wrapped output iterator.


[heading Segmentation]

Segmentations are expressed in terms of the [conceptref Consumer] concept, which is inherently
very similar to the [conceptref Pipe] concept excepts it doesn't perform any kind of transformation,
it just reads part of the input.
As a matter of fact, a =Pipe= can be converted to =Consumer= using [classref boost::pipe_consumer].

Segmentation may be done either by using the appropriate =Consumer= directly, or by using the
[classref boost::consumer_iterator] template to adapt the range into a
read-only range of subranges.

Additionally, the [conceptref BoundaryChecker] concept may prove useful to tell whether
a segment starts at a given position; a =Consumer= may also be defined
in terms of it using [classref boost::boundary_consumer].

The naming scheme is as follows:

* [classref boost::unicode::u8_boundary] is a =BoundaryChecker= that tells whether a position is the start of a code point in a range of UTF-8 code units.
* [classref boost::unicode::grapheme_boundary] is a =BoundaryChecker= that tells whether a position is the start of a grapheme cluster in a range of code points.
* [funcref boost::unicode::u8_bounded] adapts its input range in UTF-8 into a range of ranges of code units, each range being a code point.
* [funcref boost::unicode::grapheme_bounded] adapts its input range in UTF-32 into a range of ranges of code points, each range being a grapheme cluster.
* [funcref boost::unicode::u8_grapheme_bounded] adapts its input range in UTF-8 into a range of code units, each range being a grapheme cluster.

[heading UTF type deduction with SFINAE]

Everytime there are two versions for a function or class, one for UTF-8 and
the other for UTF-16, and deducing which type of UTF encoding to use is
possible, additional ones are added that will automatically forward to it.

The naming scheme is as follows:

* [funcref boost::unicode::utf_decode] either calls [funcref boost::unicode::u8_decode] or [funcref boost::unicode::u16_decode]
depending on the =value_type= of its input range.
* [classref boost::unicode::utf_boundary] either behaves like
[classref boost::unicode::u8_boundary] or [classref boost::unicode::u16_boundary]
depending on the =value_type= of the input ranges passed to =ltr= and =rtl=.

[endsect]

[section Composition and Normalization]

Normalized forms are defined in terms of certain decompositions applied
recursively, followed by certain compositions also applied recursively,
and finally canonical ordering of combining sequences.

A decomposition being a conversion of a single code point into several
and a composition being the opposite conversion, with exceptions.

[heading Decomposition]
The Unicode Character Database associates with code points certain decompositions,
which can be obtained with [funcref boost::unicode::ucd::get_decomposition],
but does not include Hangul syllable decompositions since those can be easily
procedurally generated, allowing space to be saved.

The library provides [classref boost::unicode::hangul_decomposer], a
[conceptref OneManyPipe] to decompose Hangul syllables.

There are several types of decompositions, which are exposed by
[funcref boost::unicode::ucd::get_decomposition_type], most importantly
the canonical composition is obtained by applying both the Hangul
decompositions and the canonical decompositions from the UCD, while the
compatibility decomposition is obtained by applying the Hangul decompositions
and all decompositions from the UCD.

The [classref boost::unicode::decomposer] template allows to generate a
model of [conceptref OneManyPipe] that performs any decomposition.

[heading Composition]
Likewise, Hangul syllable compositions are not provided by the UCD and
are implemented by [classref boost::unicode::hangul_composer] instead.

Some distinct code points may have the same decomposition, so certain
decomposed forms are preferred. That is why an exclusion table is also
provided by the UCD.

[heading Recursive application and Normalization]
TODO
[endsect]

[section String searching algorithms]
The library provides string searching algorithms that are able to search
for a range of code units within another range of code units, with the
search lying on the right boundaries specified as a parameter.

This effectively allows to perform the search at the code unit, code point,
grapheme or even word level.

Different approaches to string searching are being considered:

* [conceptref Consumer]-based, the classic algorithms are simply adapted to segmented ranges.
* [conceptref BoundaryChecker]-based, the classic algorithms are run, then false positives
that don't lie on the right boundaries are discarded.

Note that the algorithms are not responsible for normalizing input, and that the code points
will need to be the same for them to compare equal.

[endsect]

[endsect]

[section Unicode in source files]

It is often quite useful to embed strings of text directly into source
files, and C++, as of the 2003 standard, provides the following ways to
do so: string literals, wide string literals, character and
wide character literals, and finally type lists that form
compile-time strings. One has to be aware, however, of the various
portability issues associated with character encoding within source
files.

The first limitation is that of what character encoding the compiler
expects the source files to be in, the source character set. The second
one is what character encoding narrow and wide string literals will
have at runtime: the execution character set, which may be different for
narrow and wide strings.

Indeed, while certain compilers remain encoding-agnostic as long as the
source is ASCII-compatible, certain will convert the string literals
from the source character set to their appropriate execution character
sets.
This is the case of MSVC which, when it detects a source file is in
UTF-8 or UTF-16, will convert narrow string literals to ANSI and wide
string literals to UTF-16. Furthermore, if it doesn't detect the
character encoding of the source file, it will still convert wide string
literals from ANSI to UTF-16 while leaving narrow ones untouched.

Also, regardless of whether the compiler detects the character
encoding of the source file or not, Unicode escape sequences,
[^\u['xxxx]] and [^\U['xxxxxxxx]], will be translated to the execution
character set of the literal type they're embedded in.
Unfortunately, that makes them unusable portably within narrow strings,
as there is no way to set the narrow execution character set to UTF-8
with MSVC, and UTF-8 is the way Unices are going.

Finally, wide characters are not well defined. In practice, they're
either UTF-16 or UTF-32 code units, but their use is quite discouraged
as the size of =wchar_t= is very variable: 16 bits on MS Windows,
usually 32 bits on Unices.
Nevertheless, in the lack of UTF-16 and UTF-32 literals that are coming
with C++0x, wide string literals are probably the closest thing there is
to native Unicode in the compiler. The library tools that automatically
deduce the UTF encoding based on the size of the value
type will therefore work as expected as they will expect =wchar_t= to
represent either UTF-16 or UTF-32 depending on its size.

Alternatively, compile-time strings may be used, which allow a great
deal of flexibility as arbitrary character encoding conversion may
then be performed at compile-time, but which remain more verbose to
declare and increase compilation times.

We can then infer certain guidelines to write Unicode data within
C++ source files in a portable way while taking a few reasonable
assumptions.

[heading Portability guidelines]

* Source file encoding: use UTF-8 without a Byte Order Mark or use
ASCII. This ensures most compilers will run in an encoding-agnostic mode
and not perform translations, plus most compilers only support
ASCII-compatible input.
* Narrow character literals: use ASCII only; if you use escape sequences
such as =\x= treat the data you're inputting as UTF-8 code units. Ban
=\u= and =\U=.
* Narrow string literals: freely input UTF-8; if you use escape
sequences such as =\x= treat the data you're inputting as UTF-8 code
units. Ban =\u= and =\U=.
* Wide character literals: use ASCII only; if you use escape sequences
such as =\x= treat the data you're inputting as UTF-32 code units,
but don't input anything higher than =0xD800=. Use heavily =\u= but ban
=\U=.
* Wide string literals: use ASCII only; if you use escape sequences
such as =\x= treat the data you're inputting as UTF-32 code units,
but don't input anything higher than =0xD800=. Use heavily both =\u= and
=\U=.

[heading Compile-time strings]

Option one is to use =boost::mpl::string= as a UTF-8 compile-time
string. Its support for multi-char character literals allows it to not
be too verbose, and it can be coupled with [classref boost::unicode::string_cp]
to insert Unicode code points in stead of the Unicode escape sequences.
Any non-ASCII character shall be put as its own character literal. Note
multi-char character literals require =int= to be at least 32 bits
however.

A second option is to use =boost::mpl::u32string= as a UTF-32 compile-time
string, and use [classref boost::unicode::static_u8_encode] or
[classref boost::unicode::static_u16_encode] to eventually encode it
at compile-time to UTF-8 or UTF-16. =boost::mpl::u16string= may also be
used to directly input UTF-16.
However, none of these two sequence types provide support for easier
declaration with multi-char character literals.

Then, the =boost::mpl::c_str= meta-function may be used to convert
any compile-time string into a zero-terminated equivalent.

[heading Example]

See the [link unicode.examples.source_input source_input] example for
demonstrations.

[endsect]

[section Unicode String type]

A Unicode string type may be added in future versions that maintains
its data in Normalized C form on top of a valid UTF encoding.

[endsect]

[section Examples]

[section convert]
[import ../example/convert.cpp]
[convert]
[endsect]

[section characters]
[import ../example/characters.cpp]
[characters]
[endsect]

[section compose]
[import ../example/compose.cpp]
[compose]
[endsect]

[section search]
[import ../example/search.cpp]
[search]
[endsect]

[section source_input]
[import ../example/source_input.cpp]
[source_input]
[endsect]

[endsect]

[xinclude autodoc1c.xml]
[xinclude autodoc2.xml]

[section Acknowledgements]

Eric Niebler for mentoring this project, John Maddock for contributing preliminary on-the-fly UTF conversion, Graham Barnett and Rogier van Dalen for their work
on Unicode character properties.

Beman Dawes and other members of the mailing list for their suggestions and support.

[endsect]
