[library Unicode
    [quickbook 1.3]
    [version 0.1 preview 4]
    [authors [Gaunard, Mathias]]
    [copyright 2009-2010 Mathias Gaunard]
    [category string-text]
    [purpose Internationalized text handling in C++ with Unicode]
    [license
        Distributed under the Boost Software License, Version 1.0.
        (See accompanying file LICENSE_1_0.txt or copy at
        [@http://www.boost.org/LICENSE_1_0.txt])
    ]
]

[/ Some links]

[def __note__           [$images/note.png]]
[def __alert__          [$images/alert.png]]
[def __tip__            [$images/tip.png]]

[def __unicode_std__    [@http://www.unicode.org/versions/latest/ Unicode Standard]]
[def __tr10__           [@http://unicode.org/reports/tr10/ Technical Standard #10 - Unicode Collation Algorithm]]
[def __tr15__           [@http://unicode.org/reports/tr15/ Annex #15 - Normalization Forms]]
[def __tr29__           [@http://unicode.org/reports/tr29/ Annex #29 - Text Segmentation]]
[def __boost_range__    [@http://boost.org/libs/range/index.html Boost.Range]]

[section Preface]

[:Unicode is the industry standard to consistently represent and manipulate text across most of the world's writing systems.]


[heading Description]

This library aims at providing the foundation tools to accurately represent and deal with natural text in C++ in a portable
and robust manner, so as to allow internationalized applications, by implementing parts of the __unicode_std__.

This library is environment-independent and deliberately chooses not to rely to the standard C++ locale facilities
as well as the standard string facilities, judged ill-suited for Unicode. It does, however provide tools that can be used with them.

The current version is locale-agnostic, but a subsystem for tailored locale behaviour may be added in the future.

[warning Boost.Unicode is a library in development and is not part of Boost.]

[heading How to use this manual]

Some icons are used to mark certain topics indicative of their relevance. These
icons precede some text to indicate:

[table Icons
    [[Icon]         [Name]          [Meaning]]
    [[__note__]     [Note]          [Information provided is auxiliary but will
                                     give the reader a deeper insight into a specific
                                     topic. May be skipped.]]
    [[__alert__]    [Alert]         [Information provided is of utmost importance.]]
    [[__tip__]      [Tip]           [A potentially useful and helpful piece of
                                     information.]]
]

[endsect]

[section Motivation]

Most software applications need, at a moment or another, to deal with
text in a natural language, to analyze it or perform certain operations
on it.

Solutions to represent and deal with text in languages based on a latin
alphabet appeared very early in the history of software, but not only
did they not fulfill all linguistics needs, they only worked for a subset
of the languages they were supposed to deal with.
Solutions to deal with text in other languages then emerged after that
over the years, but were still restricted to a specific language.

Since the late 80's, effort has been made to create an universal solution to
represent and deal with text in any language, in particular due to
internationalization considerations.

The Unicode standard was thus born, which not only provides means to
encode text from any natural language ever created into a digital form,
but also categorizes characters, allows to identify graphemes, words,
sentences and lines, and allows to perform case conversion and to sort
text, either in a language-agnostic or language-tailored manner.

This library aims at providing the mechanisms to deal with text in a
natural language, in a language- and platform-agnostic way, using the
foundations of the Unicode standard.

In particular, ability to have a proper abstraction of what a character
is is deemed important.

[endsect] [/Motivation]

[section Introduction to Unicode]

[section Notion of character]

[variablelist Language terms:
    [[Character] [a unit of information that roughly corresponds to a grapheme, grapheme-like unit, or symbol, such as in an alphabet or syllabary in the written form of a natural language.]]
    [[Grapheme] [the fundamental unit in written language.]]
    [[Glyph] [an element of writing in a natural language, visual variants of the abstract unit known as grapheme.]]
]

As can be seen, what a character is is loosely defined. There are various
levels at which to approximate it within Unicode, which are further
explained below.

[variablelist Unicode terms:
    [[Code unit] [the unit in terms of which the string of text is encoded in, in one of the Unicode transformation formats.]]
    [[Code point] [a numerical value, encoded as potentially several code units, that is part of the Unicode code space, i.e. the set of all characters it maps and maintains.]]
    [[Combining character sequence] [a sequence of code points that is the unit for the Unicode composition and decomposition processes]]
    [[Grapheme cluster] [a cluster of code points that form a grapheme.]]
]

The general idea is that "character" in Unicode usually refers to code points,
and higher levels of abstractions are usually referred to as "abstract
characters", or directly with their actual name.

[endsect] [/Definitions]


[section Character set]
The Unicode character set is a mapping that associates *code points*, which are integers, to characters for any writing system or language.

As of version 5.1, there are 100,507 characters, requiring a storage capacity of 17 bits per code point. The unicode standard however
also reserves some code ranges, known as planes, meaning it really requires a storage capacity of 21 bits.

Since microprocessors usually deal with integers whose capacity are multiples of 8 bits, a naive usage would be to use 32 bits per code point,
which is quite a waste, especially since most daily-used characters lie in the Basic Multilingual Plane, which fits on 16 bits.

That is why variable-width encodings were designed, where each code point is represented by a variable number of *code units*, formely also known as code values.
[endsect]

[section Encodings]

The UTF-X family of encodings encode a single *code point* into a variable number of *code units*, each of which does X bits.

[heading UTF-32]

This encoding is fixed-width, each code unit is simply a code point.

This encoding isn't really recommended for internal representations other
that for use with algorithms that strictly require random access of code points.

[heading UTF-16]

Every code point is encoded by one or two code units. If the code point lies within the BMP, it is represented by exactly that code point.
Otherwise, the code point is represented by two units which both lie in the surrogate category of Unicode code points.

This is the recommended encoding for dealing with Unicode internally for general purposes, since it has fairly low processing overhead
compared to UTF-8 and doesn't waste as much memory as UTF-32.

[heading UTF-8]

This encoding was designed to be compatible with legacy, 8-bit based, text management.

Every code point within ASCII is represented as exactly that ASCII character, others are represented as a variable-sized sequence from
two to four bytes, all of which are non-ASCII.

This encoding is popular for data storage and interchange, but can also be useful for compatibility with byte-oriented string manipulation.

[endsect]

[section Combining character sequences]

A *non-combining code point* may be followed by an arbitrary number of *combining code points* to form a single *combining character sequence*, which is really a composite character.

Certain characters are only available as a combination of multiple code points, while some, the ones that are expected to be the most used,
are also available as a single precomposed code point. The order of the combined code points may also vary, but all code points combinations
leading to the same abstract character are still canonically equivalent.

While a combining character sequence can be arbitrarily big, the Unicode standard also introduces the concept of a *stream-safe string*, where
a combining character sequence is at most 31 code points long, which is largely above what is sufficient for any linguistic use.

[endsect]

[section Grapheme clusters]

Another yet higher-level abstraction of character is that of a *grapheme cluster*, i.e. a cluster of *code points* that constitutes a *grapheme*.
All *combining character sequences* are graphemes, but there are other sequences of *code points* that are as well; for example =\r\n= is one.

For certain classes of applications, such as word processors, it can be important to operate at the *grapheme* level rather than at the *code point* or
*combining character sequence* one, as this is what the document is composed in terms of.

[endsect]

[section Normalization]

The Unicode standard defines four normalized forms in __tr15__ where *combining character sequences* are either fully compressed or decompressed,
using either canonical or compatiblity decompositions.

The Normalized Form C is of a great interest, as it compresses every grapheme so that is uses as few code points as possible. It's also
the one that operates best with legacy systems unaware of combining character sequences, font rendering systems and is also
the normalized form assumed by the XML standard.

On the other hand, the Normalized Form D uses a lot more space, but is more efficient to compute and to use
when concatenating combining characters to a string while maintaining the form.
[endsect]

[section Other operations]
The Unicode standard also specifies various features such as a collation algorithm in __tr10__ for comparison and ordering of strings with
a locale-specific criterion, as well as mechanisms to iterate over words, sentences and lines in __tr29__.

Those features are not implemented by the current version of the library.
[endsect]

[section Character properties]

Unicode also provides a database of character properties called the Unicode Character Database (UCD), which consists of a set of files describing
the following properties:

* Name.
* General category (classification as letters, numbers, symbols, punctuation, etc.).
* Other important general characteristics (white space, dash, ideographic, alphabetic, non character, deprecated, etc.).
* Character shaping (bidi category, shaping, mirroring, width, etc.).
* Case (upper, lower, title, folding; both simple and full).
* Numeric values and types (for digits).
* Script and block.
* Normalization properties (decompositions, decomposition type, canonical combining class, composition exclusions, etc.).
* Age (version of the standard in which the code point was first designated).
* Boundaries (grapheme cluster, word, line and sentence).
* Standardized variants.

The database is useful for Unicode implementation in general, as it is the base for most algorithms, but can also be of interest to the library user that wants to
implement facilities not provided by the library core.

[endsect]
[endsect]

[section Overview]

[section Components]
Part of this library is header-only, while part requires to link against a library.

The library provides the following header-only components:

* the type char16 and char32, suitable for encoding UTF-16 and UTF-32 respectively,
* a comprehensive converter and segmenter framework, which allows among others to convert a range as it is iterated or to convert a file stream using a codecvt facet,
* converters between the various UTF encodings and the locale character sets,
* compile-time unicode strings and compile-time UTF converters,
* converters that compose or decompose Hangul characters.

The following features are available by linking against the library:

* a Unicode character database, which, for each Unicode code point, provides many properties,
* converters for decomposition, composition, and normalization,
* functions to concatenate normalized ranges,
* segmenters for graphemes, and in the close future words, sentences and line breaks.

This library defines the concepts of [conceptref Converter] and [conceptref Segmenter], which are mechanisms to arbitrarily convert or segment ranges of data, expressed as pairs of iterators.
The Converter and Segmenters framework allows to perform these either eaglery
[endsect] [/ Scope]

[section Organization]

[caution The organization of headers may change in the future in order to
improve compile times.]

[variablelist Main headers
    [[[headerref boost/cuchar.hpp]] [Primitive types for UTF code units.]]
    [[[headerref boost/unicode/utf.hpp]] [Conversion between UTF encodings.]]
    [[[headerref boost/unicode/static_utf.hpp]] [Compile-time conversion between UTF encodings.]]
    [[[headerref boost/unicode/graphemes.hpp]] [Functions to iterate and identify graphemes.]]
    [[[headerref boost/unicode/compose.hpp]] [Functions to compose, decompose and normalize unicode strings.]]
    [[[headerref boost/unicode/cat.hpp]] [Functions to concatenate normalized strings while maintaining a normalized form.]]
    [[[headerref boost/unicode/search.hpp]] [Utility to adapt Boost.StringAlgo finders to discard matches that lie on certain boundaries.]]
    [[[headerref boost/unicode/ucd/properties.hpp]] [Access to the properties attached with a code point in the Unicode Character Database.]]
]

[endsect] [/ Organization]

[section Linking]

As has been stated in [link unicode.introduction_to_unicode.character_properties Introduction to Unicode], several Unicode algorithms require the usage of a large
database of information which, as of the preview 4 of this library, is 500 KB on x86 when stripped. Note that at the current stage of development, the database does not contain
everything one might need to deal with Unicode text, so it may grow in the future.

Features that can avoid dependency on that database do so; so it is not required for UTF conversions for example, that are purely header-only.

[heading UCD generation]

The Unicode Character Database can be generated using a parser present in the source distribution of this library to analyze
[@http://www.unicode.org/Public/ the data provided by Unicode.org].

Note however that the parser itself needs to be updated to be made aware of new proprieties values; otherwise those properties
will fallback to the default value for that property and the parser will issue a warning.

[heading Binary compatibility]

The UCD is fully backward compile, and unknown property values returned by the linked library will automatically be converted to
the default value for that property. This is consistent with how new values are introduced in the standard.

[heading Alternate databases]

Future versions of this library may provide alternate implementations of this database as a thin layer over a database provided by
another library or environment to prevent duplication of data. All this should be entirely binary compatible, and using one database
or another should just be a drop-in replacement of a shared object.

[endsect] [/ Linking]
[endsect] [/ Overview]

[section Converters and Segmenters]
[section Concepts]

Two concepts are of utmost importance within this library, the [conceptref Segmenter]
concept, which is used for segmentation of text, and, more importantly,
the [conceptref Converter] concept, which is used for conversion, including
transcoding and normalization.

[heading Segmenter]
A model of the [conceptref Segmenter] concept is a class that takes an
input range, specified as two iterators, and consumes it left-to-right
or right-to-left, modifying the appropriate iterator as it advances.

Semantically, a right-to-left consuming done after a left-to-right consuming
should restore the original position. Indeed, both primitives need to
be provided in a symmetric way in order to implement bidirectional
iteration.

Here is an example of a segmenter that consumes one element in a range
of integers:
``struct element_segmenter
{
    typedef int input_type;

    template<typename In>
    void ltr(In& begin, In end)
    {
        return ++begin;
    }
    
    template<typename In>
    void rtl(In begin, In& end)
    {
        return --end;
    }
};``

A model of the [conceptref Segmenter] concept may then be used to segment
a range, either by calling manually, or by using [funcref boost::adaptors::segment],
which returns a [classref boost::segmented_range] that adapts the range into
a range of subranges.

With the above example, there would be as many subranges as elements, and
each subrange would be one element.

[heading BoundaryChecker]
A model of the [conceptref BoundaryChecker] concept is a function object
that takes three iterators, the begin, the end, and a position, and that
returns whether the position lies on a particular boundary.

Here is an example of a boundary checker that tells whether a position
is at the end of an increasing sequence of numbers.
``struct increasing_boundary
{
    typedef int input_type;
    
    template<typename In>
    bool operator()(In begin, In end, In pos)
    {
        return *boost::prior(pos) > *pos;
    }
};``

A model of the [conceptref BoundaryChecker] concept may then be used to test
if a position is the right boundary to apply a converter, such as needed by
codecvt facets, or to define a [conceptref Segmenter] using [classref boost::boundary_segmenter].

With the above eample, a segmenter created from this boundary checker
applied to the sequence \[1, 4, 8, 2, 2, 1, 7, 4\] would result in \[ \[1, 4, 8\], \[2, 2\], \[1, 7\], \[4\] ]. 

[heading Converter]
A model of the [conceptref Converter] concept is a class that takes an input
range, specified as two iterators, consumes it left-to-right
or right-to-left, modifying the appropriate iterator as it advances,
writes some elements to an output iterator, and returns it.

In terms of semantics, not only does the consuming need to be symmetric,
but the output shall also be the same for a given consumed subrange,
whatever the consuming direction.
Furthermore, the output shall always be ordered left-to-right, even when
applying the conversion right-to-left.

Here is an example of a converter that converts two adjacent numbers into the
two numbers reversed, in a range of integers that must have an
even number of elements; indeed, for the two operations to be symmetric
here, there is not really another way.
``struct reverse2_convert
{
    typedef int input_type;
    typedef int output_type;
    typedef mpl::int_<2> max_output;

    template<typename In, typename Out>
    Out ltr(In& begin, In end, Out out)
    {
        int i = *begin++;
        if(begin == end)
            throw std::out_of_range("unexpected end");
            
        *out++ = *begin++;
        *out++ = i;
        return out;
    }
    
    template<typename In, typename Out>
    Out rtl(In begin, In& end, Out out)
    {
        *out++ = *--end;
        if(end == begin)
            throw std::out_of_range("unexpected begin");
            
        *out++ = *--end;
        return out;
    }
};``

A model of the [conceptref Converter] concept may then be used to perform
a many-to-many conversion on a whole range, be it eagerly (by calling
repeatly the converter) or lazily (be evaluating it step by step as an
iterator adapter is advanced).

The [funcref boost::convert] function provides the former, while the
[funcref boost::adaptors::convert] function which returns a range in terms of
[classref boost::converted_range] provides the latter.

With the above example, the range [1, 2, 3, 4] would be converted
to [2, 1, 4, 3].

[heading OneManyConverter]

Additionally, there is a refinement of the [conceptref Converter] concept named
[conceptref OneManyConverter], where one element is converted to many.

This allows avoiding managing iterator advancement; the converter can be
defined as a single function that takes a value, an output iterator,
and returns it.

[endsect] [/ Concepts]

[section Converting and segmenting]

[heading Conversion]
Conversions can be applied in a variety of means, all generated from using
the [conceptref Converter] concept that performs one step of the conversion:

* Eager evaluation, which simply
loops the =Converter= until the whole input range has been treated.
* Lazy evaluation, where a new range is returned that wraps the input range
and converts step-by-step as the range is advanced. The resulting range is
however read-only. It is implemented in terms of [classref boost::converted_range].
* Lazy output evaluation, where an output iterator is returned that wraps the output
and converts every pushed element with a [conceptref OneManyConverter]. It is implemented in terms
of [classref boost::convert_output_iterator].

[heading Segmentation]
Segmentations are expressed in terms of the [conceptref Segmenter] concept, which is inherently
very similar to the [conceptref Converter] concept except it doesn't perform any kind of transformation,
it just reads part of the input.
As a matter of fact, a =Converter= can be converted to =Segmenter= using [classref boost::converter_segmenter].

Segmentation may be done either by using the appropriate =Segmenter= directly, or by using the
[classref boost::segmented_range] template to adapt the range into a
read-only range of subranges.

Additionally, the [conceptref BoundaryChecker] concept may prove useful to tell whether
a segment starts at a given position; a =Segmenter= may also be defined
in terms of it using [classref boost::boundary_segmenter].

[endsect]

[section Combining converters]
While it is possible to apply a converter after another, be it with [funcref boost::convert] or by using [classref boost::converted_range],
it is not generally possible to define a converter that is the combination of two others.

Indeed, a [conceptref Converter] defines a *step* of a conversion, so it becomes difficult to define what the step of a combined conversion
is if the two steps it tries to combine are mismatched or overlap.

There are therefore two limited ways to define a converter that is the combination of two others:

* [classref boost::multi_converter] applies a step of the first converter, then applies the second converter step by step on its output until
it is completely consumed. It only works as expected if the second converter expects less input than the first one outputs in a step.
It doesn't work, for example, to apply a [classref boost::unicode::normalizer] after a [classref boost::unicode::utf_decoder], because each
step of the normalizer will only be run on a codepoint, but works to normalize then encode.
* [classref boost::converted_converter] applies a step of the second converter, passing it input that has been adapted with [classref boost::converted_range].
Unfortunately, since it needs to advance the original input iterator, this cannot work unless the the first converter only ever outputs 1 element.
As a result it works fine to decode then normalize, but not the other way around.

[heading Stability by concatenation]
For some converters, applying the converter on a range of data then on another, and concatenating the results is not the same as
applying the converter once on the concatenated data.
In particular, the Unicode decomposition and composition processes are not stable by concatenation.

Such converters will not work properly when used as the first parameter to [classref boost::multi_converter], and their existence is part of the
rationale for converters not to emit special "partial" states indicating they're lacking input.

[endsect]

[section Codecvt facets]
A codecvt facet is a facility of the standard C++ locales subsystem, that can describe a left-to-right two-way conversion between two encodings
of data.

Standard file streams are imbued with a locale, and make use of the codecvt facet attached to said locale to perform conversion
between the data they receive and give to the stream user, the so-called "internal" format, and the underlying "external" format of the file,
as is manipulated by the underlying, =char=-based, filebuf.
Unfortunately, it appears it is only possible to use this mechanism with codecvt facets that have =char= as external and either
=char= or =wchar_t= as internal, but C++0x may improve the situation.

To use [classref boost::converter_codecvt], which allows to build a codecvt facet from converters, you will need two [conceptref Converter]s, one for each direction, as well as two [conceptref BoundaryChecker]s.
Indeed, as codecvt facets are passed arbitrary input buffers, there needs to be a way to tell what is the right boundaries to apply the steps on.
An alternative would be to try to apply a step and try again if there was an error due to incomplete data. This is however not sufficient for
converters that are not stable by concatenation.

You may also build converters out of codecvt facets with [classref boost::codecvt_in_converter] or [classref boost::codecvt_out_converter], or
directly convert locales to UTF-32 with [classref boost::unicode::locale_decoder] or [classref boost::unicode::locale_encoder].

[import ../test/iterator/test_codecvt.cpp]
[test_codecvt]

[endsect]

[endsect] [/ Converters and Segmenters]

[section User's Guide]

[section UTF converters and segmenters]

This library provides two kinds of operations on bidirectional ranges:
conversion (e.g. converting a range in UTF-8 to a range in UTF-32) and
segmentation (i.e. demarcating sections of a range, like code points,
grapheme clusters, words, etc.).

[heading Conversion]
The naming scheme of the utilities are as follows, here is an example
what is provided to convert UTF-32 to UTF-8:

* [classref boost::unicode::u8_encoder] is a model of the =OneManyConverter= concept.
* [funcref boost::unicode::u8_encode] is an eager encoding algorithm.
* [funcref boost::unicode::adaptors::u8_encode] returns a range adapter that does on-the-fly encoding.
* [funcref boost::unicode::adaptors::u8_encode_output] returns an output iterator adapter that will encode its elements before forwarding them to the wrapped output iterator.

[note The library considers a conversion from UTF-32 an "encoding", while a conversion
to UTF-32 is called a "decoding".
This is because code points is what the library mainly deals with, and UTF-32 is a sequence of code points.]

[heading Segmentation]
The naming scheme is as follows:

* [classref boost::unicode::u8_boundary] is a =BoundaryChecker= that tells whether a position is the start of a code point in a range of UTF-8 code units.
* [classref boost::unicode::grapheme_boundary] is a =BoundaryChecker= that tells whether a position is the start of a grapheme cluster in a range of code points.
* [funcref boost::unicode::adaptors::u8_segment] adapts its input range in UTF-8 into a range of ranges of code units, each range being a code point.
* [funcref boost::unicode::adaptors::grapheme_segment] adapts its input range in UTF-32 into a range of ranges of code points, each range being a grapheme cluster.
* [funcref boost::unicode::adaptors::u8_grapheme_segment] adapts its input range in UTF-8 into a range of code units, each range being a grapheme cluster.

[heading UTF type deduction with SFINAE]
Everytime there are two versions for a function or class, one for UTF-8 and
the other for UTF-16, and deducing which type of UTF encoding to use is
possible, additional ones are added that will automatically forward to it.

The naming scheme is as follows:

* [funcref boost::unicode::utf_decode] either behaves like [funcref boost::unicode::u8_decode], [funcref boost::unicode::u16_decode]
depending on the =value_type= of its input range.
* [classref boost::unicode::utf_boundary] either behaves like
[classref boost::unicode::u8_boundary] or [classref boost::unicode::u16_boundary]
depending on the =value_type= of the input ranges passed to =ltr= and =rtl=.

[tip Not only UTF-8 and UTF-16 are recognized by UTF type deduction, UTF-32 is as well.]

[endsect] [/UTF converters and segmenters]

[section Composition and Normalization]

Normalized forms are defined in terms of certain decompositions applied
recursively, followed by certain compositions also applied recursively,
and finally canonical ordering of combining character sequences.

A decomposition being a conversion of a single code point into several
and a composition being the opposite conversion, with exceptions.

[heading Decomposition]
The Unicode Character Database associates with code points certain decompositions,
which can be obtained with [funcref boost::unicode::ucd::get_decomposition],
but does not include Hangul syllable decompositions since those can be easily
procedurally generated, allowing space to be saved.

The library provides [classref boost::unicode::hangul_decomposer], a
[conceptref OneManyConverter] to decompose Hangul syllables.

There are several types of decompositions, which are exposed by
[funcref boost::unicode::ucd::get_decomposition_type], most importantly
the canonical composition is obtained by applying both the Hangul
decompositions and the canonical decompositions from the UCD, while the
compatibility decomposition is obtained by applying the Hangul decompositions
and all decompositions from the UCD.

[classref boost::unicode::decomposer], model of [conceptref Converter]
allows to perform any decomposition that matches a certain mask, recursively,
including Hangul ones (which are treated as canonical decompositions),
and canonically orders combining sequences as well.

[heading Composition]
Likewise, Hangul syllable compositions are not provided by the UCD and
are implemented by [classref boost::unicode::hangul_composer] instead.

Some distinct code points may have the same decomposition, so certain
decomposed forms are preferred. That is why an exclusion table is also
provided by the UCD.

The library uses a pre-generated prefix tree (or, in the current
implementation, a lexicographically sorted array) of all canonical
compositions from their fully decomposed and canonically ordered form to
identity composable sequences and apply the compositions.

[classref boost::unicode::composer] is a [conceptref Converter] that uses that
tree as well as the Hangul compositions.

[heading Normalization]
Normalization can be performed by applying decomposition followed by
composition, which is what the current version of [classref boost::unicode::normalizer]
does.

The Unicode standard however provides as well quick-check properties to
avoid that operation when possible, but the current version of the library
does not support that scheme at the moment.

[heading Concatenation]
Concatenating strings in a given normalization form does not guarantee the result
is in that same normalization form if the right operand starts with a combining
code point.

Therefore the library provides functionality to identity the boundaries where
re-normalization needs to occur as well as eager and lazy versions of the
concatenation that maintain the input normalization.

Note concatenation with Normalization Form D is slightly more efficient as it only
requires canonical sorting of the combining character sequence placed at
the intersection, while Normalization Form C requires the sequence be renormalized.

See:

* [funcref boost::unicode::cat_limits] to partition into the different sub ranges.
* [funcref boost::unicode::composed_concat], eager version with input in Normalization Form C.
* [funcref boost::unicode::adaptors::composed_concat], lazy version with input in Normalization Form C.
* [funcref boost::unicode::decomposed_concat], eager version with input in Normalization Form D.
* [funcref boost::unicode::adaptors::decomposed_concat], lazy version with input in Normalization Form D.

[endsect] [/Normalization]

[section String searching algorithms]
The library provides mechanisms to perform searches at the code unit, code point,
or grapheme level, and in the future will provide word and sentence level
as well.

Different approaches to do that are possible:

* [conceptref Converter]- or [conceptref Segmenter]-based, you may simply run classic search algorithms, such as
the ones from Boost.StringAlgo, with ranges of the appropriate elements -- those elements being able
to be ranges themselves (subranges returned by [classref boost::segment_iterator] are =EqualityComparable=).
* [conceptref BoundaryChecker]-based, the classic algorithms are run, then false positives
that don't lie on the right boundaries are discarded. This has the advantage of reducing conversion and
iteration overhead in certain situations.
The most practical way to achieve this is to adapt a =Finder= in Boost.StringAlgo with [classref boost::algorithm::boundary_finder],
and the boundary you are interested in testing, for example [classref boost::unicode::utf_grapheme_boundary].

[important You will have to normalize input before the search if you want canonically equivalent things
to compare equal.]

[endsect] [/String searching]

[endsect] [/User's Guide]

[section Examples]

[section convert]
[import ../example/convert.cpp]
[convert]
[endsect]

[section characters]
[import ../example/characters.cpp]
[characters]
[endsect]

[section compose]
[import ../example/compose.cpp]
[compose]
[endsect]

[section search]
[import ../example/search.cpp]
[search]
[endsect]

[section source_input]
[import ../example/source_input.cpp]
[source_input]
[endsect]

[endsect]

[xinclude autodoc1c.xml]
[xinclude autodoc2.xml]

[section Appendices]

[section:appendix_source Appendix A: Unicode in source files]

It is often quite useful to embed strings of text directly into source
files, and C++, as of the 2003 standard, provides the following ways to
do so: string literals, wide string literals, character and
wide character literals, and finally type lists that form
compile-time strings. One has to be aware, however, of the various
portability issues associated with character encoding within source
files.

The first limitation is that of what character encoding the compiler
expects the source files to be in, the source character set. The second
one is what character encoding narrow and wide string literals will
have at runtime: the execution character set, which may be different for
narrow and wide strings.

Indeed, while certain compilers remain encoding-agnostic as long as the
source is ASCII-compatible, certain will convert the string literals
from the source character set to their appropriate execution character
sets.
This is the case of MSVC which, when it detects a source file is in
UTF-8 or UTF-16, will convert narrow string literals to ANSI and wide
string literals to UTF-16. Furthermore, if it doesn't detect the
character encoding of the source file, it will still convert wide string
literals from ANSI to UTF-16 while leaving narrow ones untouched.

Also, regardless of whether the compiler detects the character
encoding of the source file or not, Unicode escape sequences,
[^\u['xxxx]] and [^\U['xxxxxxxx]], will be translated to the execution
character set of the literal type they're embedded in.
Unfortunately, that makes them unusable portably within narrow strings,
as there is no way to set the narrow execution character set to UTF-8
with MSVC, and UTF-8 is the way Unices are going.

Finally, wide characters are not well defined. In practice, they're
either UTF-16 or UTF-32 code units, but their use is quite discouraged
as the size of =wchar_t= is very variable: 16 bits on MS Windows,
usually 32 bits on Unices.
Nevertheless, in the lack of UTF-16 and UTF-32 literals that are coming
with C++0x, wide string literals are probably the closest thing there is
to native Unicode in the compiler. The library tools that automatically
deduce the UTF encoding based on the size of the value
type will therefore work as expected as they will expect =wchar_t= to
represent either UTF-16 or UTF-32 depending on its size.

Alternatively, compile-time strings may be used, which allow a great
deal of flexibility as arbitrary character encoding conversion may
then be performed at compile-time, but which remain more verbose to
declare and increase compilation times.

We can then infer certain guidelines to write Unicode data within
C++ source files in a portable way while taking a few reasonable
assumptions.

[heading Legacy guidelines]

The following guidelines ensure everything will go welll, regardless of
compiler or environment setup:

* Source file encoding: use UTF-8 without a Byte Order Mark or use
ASCII. This ensures most compilers will run in an encoding-agnostic mode
and not perform translations, plus most compilers only support
ASCII-compatible input.
* Narrow character literals: use ASCII only; if you use escape sequences
such as =\x= treat the data you're inputting as UTF-8 code units. Ban
=\u= and =\U=.
* Narrow string literals: freely input UTF-8; if you use escape
sequences such as =\x= treat the data you're inputting as UTF-8 code
units. Ban =\u= and =\U=.
* Wide character literals: use ASCII only; if you use escape sequences
such as =\x= treat the data you're inputting as UTF-32 code units,
but don't input anything higher than =0xD800=. Use heavily =\u= but ban
=\U=.
* Wide string literals: use ASCII only; if you use escape sequences
such as =\x= treat the data you're inputting as UTF-32 code units,
but don't input anything higher than =0xD800=. Use heavily both =\u= and
=\U=.

[heading Modern guidelines]
With C++0x introducing UTF-8, UTF-16, and UTF-32 literals, it becomes
clear the way to go is to rely on the compiler to convert from the source
character set to whatever encoding the strings should be in.
Indeed, the compiler will convert from the source character set to UTF-8
for UTF-8 literals, UTF-16 for UTF-16 literals, UTF-32 for UTF-32 literals,
the narrow execution character set for narrow literals, and the wide execution
character set for wide literals.

Assuming you can reliably ensure that all compilers recognize the same
source character set, you can make full usage of all literal types freely.
However, for UTF-8 source files, MSVC requires a BOM, while GCC requires it to not be
present. If you can accomodate this in your environment, then definitely go
for this solution, which is simpler and more powerful.

[heading Compile-time strings]

Option one is to use =boost::mpl::string= as a UTF-8 compile-time
string. Its support for multi-char character literals allows it to not
be too verbose, and it can be coupled with [classref boost::unicode::string_cp]
to insert Unicode code points instead of the Unicode escape sequences.
Any non-ASCII character shall be put as its own character literal. Note
multi-char character literals require =int= to be at least 32 bits
however.

A second option is to use =boost::mpl::u32string= as a UTF-32 compile-time
string, and use [classref boost::unicode::static_u8_encode] or
[classref boost::unicode::static_u16_encode] to eventually encode it
at compile-time to UTF-8 or UTF-16. =boost::mpl::u16string= may also be
used to directly input UTF-16.
However, none of these two sequence types provide support for easier
declaration with multi-char character literals.

Then, the =boost::mpl::c_str= meta-function may be used to convert
any compile-time string into a zero-terminated equivalent.

[heading Example]

See the [link unicode.examples.source_input source_input] example for
demonstrations.

[endsect] [/Unicode in source]

[section Appendix B: Rationale]

[heading Iterators rather than streams]
The library chooses to base itself upon iterator adapters rather than
upon streams, even though the latter were designed for conversion
facilities with buffering and can be configured with locales.

That choice was made because it is believed that the iterator and
range abstractions are more flexible and easier to deal with, and that
there are also quite more efficient.


[heading Converter concept]
Centralizing conversion into a single [conceptref Converter] model allows
eager and lazy variants of evaluation to be possible for any conversion
facility.

Lazy evaluation is believed to be of great interest since it avoids the
need for memory allocations and buffers and constructing a logic
conversion is constant-time instead of linear-time since there is no need
to actually walk the range.

Eager evaluations can remain more efficient however, and that is why they
are provided as well.

[endsect] [/Rationale]

[section Appendix C: Future Work]

[heading Non-checked UTF conversion]
The library only provides UTF conversion converts that do extensive checking
that the input is correct and that the end is not unexpectedly met.

These could be avoided when it is known that the input is valid, and
thus performance be increased. [classref boost::convert_iterator] could as
well avoid storing the =begin= and =end= iterator in such cases.

[heading Fast Normalization]
The Unicode standard provides a quick-check scheme to tell whether a string
is in a normalized form, which could be used to avoid expensive decomposition
and recomposition.

[heading Unicode String type]
Future versions of the library could provide a string type that maintains
the following invariants: valid UTF, stream-safe and in Normalization Form C.

[endsect] [/Future Work]

[section Appendix D: Acknowledgements]

I would like to thank Eric Niebler for mentoring this project
as part of the Google Summer of Code program, who provided steady help
and insightful ideas along the development of this project.

Graham Barnett and Rogier van Dalen deserve great thanks as well for
their work on Unicode character properties, most of the parser of
Unicode data was written by them.

John Maddock was also a great help by contributing preliminary on-the-fly UTF conversion
which helped the library get started, while inspiration from Phil Endecott
allowed UTF conversion code to be more efficient.

Finally, I thank Beman Dawes and other members of the mailing list for
their interest and support.

[endsect] [/Acknowledgements]

[endsect] [/Appendices]
