[section:introduction Introduction]

[section:dataflow Dataflow programming in C++ - motivation and advantages]

The [WikiDataflow] programming paradigm is based on interconnected
/components/ which process passing /data/.  Basically, data is treated
as something that originates from a source, flows through a number of
processing components that manipulate it (e.g., by changing it,
duplicating, etc.), and arrives at some final destination.  As such,
the dataflow paradigm is most suitable when developing applications that
are themselves focused on the "flow" of data.

Perhaps the most readily available examples of a
dataflow-oriented applications come from the realm of real-time
[@http://en.wikipedia.org/wiki/Signal_processing signal processing],
e.g. a video signal processor which perhaps starts with a video input,
modifies it through a number of processing components (video filters),
and finally outputs it to a video display.

[heading A motivating example]

Let's take a simple real-time camera-input-displayed-on-the-screen application.
Suppose there are three parts to the application - getting each image frame
from the camera, processing the image in some way,
and displaying it on the screen. To see how we might arrive at
a dataflow-oriented implementation of this application,
let's first begin with an imperative approach.  Such an implementation might
be structured as follows:

[$dataflow1.png]

Basically, the main program loop is a series of instructions which does
this particular job.  To take this a step further in the dataflow direction,
we note that video input libraries often provide callback
functionality which will deliver a video stream as a sequence of
image frames given at the appropriate frame rate.
With this in mind, we do the following:

# implement a function which takes an image as input.
 # the function first invokes the image processing library function that
   modifies the image as appropriate
 # the function then invokes the GUI library function that displays the image
# register the function as a callback with the camera library
# the main program loop can relax and have some coffee.

The situation now looks something like this:

[$dataflow2.png]

So now, the image library is acting as a data/signal producer - it
generates images at a certain frame rate.  And the function we
implemented seems to be a signal consumer which can take an image,
process it, and display it on the screen.  

This now employs the basic elements of the dataflow paradigm, but we could
take it even further.  Instead of just having two components, one image
signal generator and one signal consumer, how about this:

# implement a component which accepts an input image signal,
  modifies the image as appropriate, and then outputs
  a signal with the modified image
# implement a component which receives an imput image signal and displays
  it on the screen
# connect the camera library input stream to the first component
# connect the first component to the second component
# the main program loop can relax and have some tea, or even take a nap.

The big picture now looks like the following:

[$dataflow3.png]

To give you a sense of how you would do something like this using the Dataflow
library, we will present a slightly simplified example using [DataflowSignals].
Instead of processing images, we will just process numbers - but the dataflow
parts of the code are pretty much the same:

[simple_example]

A sample run produces:

[pre

0.213436
-0.49558
1.57538
-1.0592
1.83927
1.88577
0.604675
...

]

...not quite image processing, but you get the (dataflow) point :-)

[heading Advantages]

There are already many programming paradigms supported by C++ (either directly or through
additional libraries), so let's examine what the advantages of the the dataflow paradigm might be.

First of all, [*dataflow programming is not exclusive of other paradigms], so adopting the dataflow paradigm
does not hinder the use of other techniques.  In fact, in C++ it can't - since the components
themselves need to be implemented somehow, and we can't recursively define them
forever as finer and finer dataflow diagrams, the dataflow paradigm relies on other
programming techniques to do the underlying work.  Also, dataflow does not need
be used for the entire application implementation.  You can always use it
for only those parts of the application it is appropriate for, and 
"extend your fingers" from other parts of the program in order
to insert data into the dataflow, catch it on the other end, probe and adjust the components, etc.
You can think of it as working with electronic components and changing the connections, turning knobs,
flipping switches, or hanging over a circuit board and tinkering with it using, say, a multimeter and a
5V lead (just do it with care).

Second, [*dataflow promotes some good programming practices].  When developing processing components,
we have only the incoming data to deal with - with no requirements on where it is coming from.  Hence,
the developed components tend to be quite versatile and reusable.  In the above example, the image
processing component can be used with any image data generator - there is nothing inside the component
that says "get the image from the camera", or "get the image from this type of data source (where the
type is either a base class or a concept)".  It just does it's thing, no matter where the data is coming
from.

Third, when used in the right context, [*dataflow programming makes development and maintenance
very intuitive].  In the image processing example, say you don't want to process the image any more. You can
just connect the camera signal directly to the screen display and cut out
the image processing component.  Someone gives you a new video signal generator component you'd like to use as input
instead of the camera?  Just plug it in. Literally.

Fourth, [*dataflow-oriented programs can be divided between threads, processors, or computers more easily],
because the data dependencies are much more visible.  In the image processing example, say you have
the display on a different computer. You can just pass the connection to it through a network socket.
With the data flow clearly specified, it is much easier to distribute the work either manually or
even automatically (although the Dataflow library at the moment offers no such automatic functionality).

Finally, [*we are not to far from the advantages of a [@http://en.wikipedia.org/wiki/Visual_programming_language
visual programming language]], since the components and the connections have a natural graphical representation.
With a visaul development environment, programming becomes as easy as connecting components with connections
(again, the Dataflow library provides no visual programming functionality).

[heading Go with the flow?]

If you are interested in exploring the dataflow concept further using
the Dataflow library, see

* [how_to_use How to use this library and the documentation].

[endsect][/dataflow]

[section:how_to_use How to use this library and the documentation]

The Dataflow library currently has two useful parts:

* A layer providing generic dataflow support, adaptable to various dataflow
  mechanisms.
* The [DataflowSignals] layer, with a number of implemented components that
  facilitate implementation of dataflow networks using [BoostSignals] as
  a data transport mechanism.
  
There is also a (currently very experimental) [DataflowPhoenix] layer,
which uses object pointers with support for
[BoostPhoenix] actors specifying how the data is processed.

[heading Generic dataflow layer]

There are generic properties of dataflow programs which do not depend on the
data transport mechanism,
and can be exploited to develop mechanism-independent dataflow code.
The generic dataflow layer captures some of these properties, and
has been sucessfully adapted to three
different data transport mechanisms ([BoostSignals], pointers in conjunction
with [BoostPhoenix2], and [VTK] pipelines).

Currently, the most useful functionality gained by developing a Dataflow
support layer is the ability to connect components in a clean, readable
manner. As more generic code is developed on top of
the generic dataflow layer, providing a Dataflow support layer for your
favorite data transport mechanism will be more beneficial.

See the [link dataflow.future future directions] of the Dataflow library
for an idea of what might become available in the future for data transport
mechanisms with Dataflow library support.

* If you would like to implement Dataflow support for the data transport
mechanism you are working with,  see the
[link dataflow.introduction.examples.new_layer example] showing
how the [VTK] support layer was developed.
* If you are interested in developing generic code on top of the dataflow
layer, see the [concepts] documentation.

[heading Dataflow.Signals layer]

This part of the Dataflow library provides components
and connection-making capabilty which facilitates large-scale use of
Boost.Signals as a mechanism to transfer data between processing components.

For examples of how the [DataflowSignals] layer can be used, see:

* The example on developing a 
  [link dataflow.introduction.examples.distributed distributed dataflow application].
* The [DataflowSignals] documentation.

[/[heading When to use]

While the [link dataflow.introduction.dataflow dataflow] section hopefully
convinced you that there are circumstances in which a dataflow approach is 
useful, please keep in mind that there are many circumstances in which this
is not the case.

First, a dataflow approach really only makes sense when the underlying task is
really about the flow of data through the components that process it.
If you can't sketch a concise data flow diagram which truly represents
the application, the dataflow approach might not be the best option.
For example, if you are implementing a complicated algorithm which is really
about the sequence of instructions that need to be executed on the data
(rather than the data going through well-defined and self-contained
processing components), you probably should't use the Dataflow library.
If you are working on an audio or video processing application,
maybe you should.

Second, the data transport mechanism you choose should reflect the needs of
the applications closely.  Most of the functionality that the library supports
at this moment is regarding run-time configurable connections.  If you don't
need that functionality, you might be wasting resources ([DataflowPhoenix]
offers some functionality related to compile-time connectability in its
iterator_relative connections, but that is yet to trickle out into the library
as a whole).

When using signals as the data transport mechanism, remember that every signal
sent results in a function call, and if the processing
components are so minute that the cost of the function
calls overtakes the cost of the processing,
using [DataflowSignals] will cause a significant performance hit.  A similar
situation occurs with [DataflowPhoenix], where each consumer must be invoked.

To sum up, consider using the Dataflow library when:

* The application can be modeled well through the flow of data; and
* The cost of the processing shadows the cost of the function calls, and any
  unnecessary overhead caused by any connections that need to be stored.
]


[/[heading Dataflow library organization]

The design of the Dataflow library looks like this:

[xinclude dataflow_table.xml]

The layers are a bottom-up hierarchy, with dependencies
only on layers underneath.
The /support/ layer provides the necessary generic traits and functions
required for generic code to work with mechanism-specific
components.  Each type of mechanism or component must specialize the elements
of the support layer to work with the mechanism\/component.

Directly based on the support layer, we have functions like /connect/ and
/invoke/, which can be used to manipulate generic and mechanism-specific
components that the library supports.  /Operators/ are based on /connect/.


The library also offers a few generic components, which can be used
to group other components together: [producer_group], [consumer_group],
and [consumer_map].

The rest of the library is in the mechanism-specific modules [DataflowSignals]
and [DataflowPhoenix], each of which provide their own support layer, on
top of which their components are implemented.

As long as the support layer is implemented
for the mechanism/component, the component should
work seamlesly with the rest of the dataflow library.
Essentially, the support layer is a very minimal layer implementing a generic
and extensible intrusive directed graph framework.

[heading Namespace use]

Since the Dataflow library provides both a generic layer, as well as
mechanism-specific implementations, its elements are scattered over multiple
namespaces.

The fundamental user-oriented generic elements, such as `is_producer`,
`producer_category_of` etc., are located in the `boost::dataflow` namespace.
Function objects which must be specialized for different data trasport
mechanisms, such as `connect_impl`, are in the `boost::dataflow::extension`
namespace.  The connection operators are in `boost::dataflow::operators`.

On the other hand, individual data transport mechanism implementations
are located in the namespace of the data transport mechanism.  For example,
all of the [DataflowSignals] components are in the `boost::signals` namespace,
and all of the [DataflowPhoenix] components are in the `boost::phoenix`
namespace.  Furthermore, free functions such as `connect` and `invoke` are
imported into the mechanism's namespace, so that they can be used via ADL.

All of the examples shown in this documentation are assuming the use of

    using namespace boost;

[note Since there are multiple namespaces used, the documentation will
explicitly state the namespace of documented elements wherever it is
convenient.]
]

[endsect][/how_to_use]

[section Examples]
[section:new_layer Implementing support for a new mechanism]

This example shows how to implement support for a particular mechanism.
Our victim is [VTK], a 3D visualization toolkit, which uses a data pipeline
to move data from a source to the display (with possible transformations,
scene construction etc. on the way).  For example, here is an excerpt from
a VTK tutorial that sets up a whole source->render window pipeline:

```
    // allocate components
    vtkConeSource *cone = vtkConeSource::New();
    vtkPolyDataMapper *coneMapper = vtkPolyDataMapper::New();
    vtkActor *coneActor = vtkActor::New();
    vtkRenderer *ren1= vtkRenderer::New();
    vtkRenderWindow *renWin = vtkRenderWindow::New();

    // make the connections
    coneMapper->SetInputConnection( cone->GetOutputPort() );
    coneActor->SetMapper( coneMapper );
    ren1->AddActor( coneActor );
    renWin->AddRenderer( ren1 );
```

Our goal will be to simplify the connection-making code by providing
Dataflow support for VTK.  With that in place, we will be able to use the
following connection code:

```
    // make the connections
    connect(cone, coneMapper);
    connect(coneMapper, coneActor);
    connect(coneActor, ren1);
    connect(ren1, renWin);
```

or even more concisely and clearly,

```
    // make the connections
    // C++ rules prevent us from removing all of the pointer dereferencing here
    *cone >>= *coneMapper >>= *coneActor >>= *ren1 >>= *renWin;
```

[heading Next]
[link dataflow.introduction.examples.new_layer.mechanism
    Setting up the Mechanism]

[section:mechanism Setting up the Mechanism]

The first thing we'll do is create a tag for the VTK [MechanismConcept], in
namespace `boost::dataflow::vtk`.  Since there are currently no requirements
for a [MechanismConcept], this is as simple as declaring a new type to be used
as the mechanism tag:

[vtk_mechanism]

We will now use this tag in reference to the [VTK] dataflow mechanism.

[heading Next]
[link dataflow.introduction.examples.new_layer.producerconsumer
    Setting up a Producer and Consumer] 
[endsect][/mechanism]

[section:producerconsumer Setting up a ProducerPort and ConsumerPort]

Now that we have the mechanism, let's cover the basic data pipeline,
which is implemented using the
[vtkAlgorithm] class.  This class provides input and output ports. Output ports
are accessible via `GetOuptutPort` member functions, which return a proxy
object ([vtkAlgorithmOutput] `*`) for an actual output port.

In Dataflow concepts, [vtkAlgorithmOutput] can be made a [ProducerPortConcept]
- it corresponds to a single data output point. We support it as such by
creating a [PortTraitsConcept], and associating [vtkAlgorithmOutput]
with the producer category:

[vtk_algorithm_output_producer]

Now that we have a [ProducerPortConcept], we need a [ConsumerPortConcept].
[vtkAlgorithm] can accept incoming connections using the `AddInputConnection`
and `SetInputConnection` member functions.  The code below provides support
for [vtkAlgorithm] accepting connections on its default input port:

[vtk_algorithm_consumer]

[heading Next]

[link dataflow.introduction.examples.new_layer.connectable
    Making things Connectable] 

[endsect][/producerconsumer]

[section:connectable Making things Connectable]

With the pair of [ProducerPortConcept] and [ConsumerPortConcept] registered, we
can make them [ConnectableConcept] and/or [OnlyConnectableConcept].
All we need to do is specialize the implementation for the appropriate
categories:

[vtk_connect_impl_algorithm]

Connections are done through the `boost::dataflow::binary_operation` function
with either `operation::connect` or `operation::connect_only` operation tag,
and the specified mechanism. To make connecting
easier, we'll add forwarding connect and connect_only
functions in the global namespace (where vtk classes live) specific
to the vtk mechanism using the template include files for this purpose:

[vtk_specialize_connect]

[heading What we can do with what we have so far]

```
    // connect *cone to *coneMapper
    connect(*cone->GetOutputPort(), *coneMapper);
    // make *cone the only thing connected to *coneMapper
    connect_only(*cone->GetOutputPort(), *coneMapper);
```

[heading Next]
[link dataflow.introduction.examples.new_layer.proxyproducer
    Setting up a ProxyProducer] 

[endsect][/connectable]

[section:proxyproducer Setting up a ProxyProducer]

In the VTK example above, both `vtkConeSource` and `vtkPolyDataMapper`
inherit [vtkAlgorithm].  
With the [vtkAlgorithmOutput] [ProducerPortConcept] and
[vtkAlgorithm] [ConsumerPortConcept] we've set up,
we can do things like `connect(cone->GetOutputPort(), *coneMapper);`.
However, we would like to do `connect(*cone, *coneMapper)`.

To do that,
we need to make [vtkAlgorithm] a [ProducerPortConcept]. Since [vtkAlgorithm]
forms its output connections using [vtkAlgorithmOutput] objects
returned through the `GetOutputPort` member functions, and we've already
configured [vtkAlgorithmOutput] as a [ProducerPortConcept], we can make use
of the [ProxyProducerConcept] concept provided by the dataflow library.
In effect, we will make [vtkAlgorithm] delegate it's [ProducerPortConcept]
functionality to [vtkAlgorithmOutput]:

[vtk_algorithm_proxy_producer]

Now, [vtkAlgorithm] is a [ProducerPortConcept].

[heading What we can do with what we have so far]

```
    connect(*cone, *coneMapper)
```

[heading Next]
[link dataflow.introduction.examples.new_layer.filter
    Setting up a Filter (Producer+Consumer)] 

[endsect][/proxyproducer]

[section:filter Setting up a Filter (Producer+Consumer)]

In a VTK pipeline, a [vtkActor] consumes data from a [vtkMapper], and produces
data for a [vtkRenderer].  In this case, we can provide Dataflow support
for a [vtkActor] as both a [ProducerPortConcept] and a [ConsumerPortConcept] in one
fell swoop:

[vtk_actor_filter]

[heading Next]
[link dataflow.introduction.examples.new_layer.producermap
    Setting up a ProducerMap] 

[endsect][/filter]

[section:producermap Setting up a ProducerMap]

Sometimes, components can produce (or consume) multiple types of data.
For example, [vtkMapper] objects can produce data for [vtkActor] objects,
or (since [vtkMapper] inherits [vtkAlgorithm]) for other [vtkAlgorithm]
objects.  Correspondingly, connecting a [vtkMapper] to a [vtkActor] is not
the same as connecting a [vtkMapper] to another [vtkAlgorithm].

To accomodate such situations, the Dataflow library provides
[ProducerMapConcept]s, which are similar to [ProxyProducer]s but
the [ProducerPortConcept] they delegate to is keyed on the category of
[ConsumerPortConcept] they are being connected to.

[vtk_mapper_producer]

[heading Next]
[link dataflow.introduction.examples.new_layer.remaining
    Setting up the remaining components (more of the same)] 

[endsect][/producermap]

[section:remaining Setting up the remaining components (more of the same)]

[vtk_setup_rest]

[heading Next]
[link dataflow.introduction.examples.new_layer.pointers
    Supporting pointers] 

[endsect][/remaining]

[section:pointers Supporting pointers]

All of the concepts we adapted so far have been adapted for class types such
as [vtkAlgorithm] and [vtkAlgorithmOutput].  But [VTK] seems to exclusively
use pointers to class types rather than class types themselves.  So, with what
we have done so far, we can do `connect(*cone, *coneMapper)` but not
`connect(cone, coneMapper)`.  To provide support for pointers, we
simply make them adhere to the [ProxyProducerConcept] and,
[ProxyConsumerConcept], and delegate the functionality to the object they
point to.

[vtk_support_pointer]

[endsect][/pointers]



[endsect][/new_layer]

[section:distributed Implementing distributed dataflow applications using
    Dataflow.Signals and Boost.Asio]

This example shows how you can take advantage of dataflow programming
to create distributed applications.  It works as long as the data
passed in the signal is serializable using [BoostSerialization].

[DataflowSignals] provides two components
which can be used to create a producer-consumer connection between two
computers: [socket_sender] and [socket_receiver].  As long as we have
a network socket set up between two computers, we can do the following to
set up a connection between a [SignalProducerConcept] on one computer with a
[SignalConsumerConcept] on another:

* On the [SignalProducerConcept]'s computer, construct a [socket_sender] of
the appropriate `Signature` with the given socket.
[connect] the [SignalProducerConcept] to the [socket_sender].
* On the [SignalConsumerConcept]'s computer, construct a [socket_receiver] of
the appropriate `Signature` with the given socket.
[connect] the [socket_receiver] to the [SignalConsumerConcept].

That's it.  Now, every signal sent out of the [SignalProducerConcept] should
be received by the [SignalConsumerConcept].

The following is a modification of the example from the
[link dataflow.introduction.dataflow motivation section] to a dataflow
network that straddles a network socket:

[simple_distributed_example]

A sample run produces:

[pre
0.213436
-0.49558
1.57538
-1.0592
1.83927
1.88577
...
]

[endsect][/distributed]

[endsect][/examples]

[section:linking How to link]

The generic Dataflow support layer is header-only, and relies only on other
Boost header-only libraries (MPL, enable_if, and small parts of fusion).

The [DataflowSignals] layer is dependent on the [BoostSignals] library,
which must be built and linked.  A few of the components ([socket_sender]
and [socket_receiver]) are also dependent on [BoostAsio], which depends on
the System library which must also be built and linked.  A few other components
([mutex] and [condition]) are dependent on [BoostThreads], which has to be
linked as well.

[endsect][/linking]

[endsect][/introduction]
